{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/Jonas-Metz-verovis/verovis_Coding_Challenge/blob/main/02_Feature_Engineering.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introduction - Coding Challenge #2 - Feature Engineering\n",
    "\n",
    "#### Today's Coding Challenge focuses on feature engineering. We have already derived the following dataset for you from Coding Challenge #1. Your task will be to create new features and determine their influence on the target variable and of course predict the Rating of the Restaurant. The Challenge will be scored based on:\n",
    "\n",
    "1.  The Prediction Model's Test Accuracy Score\n",
    "1.  The verbal Explanations for specific Processing/Modeling Choices\n",
    "1.  The Readability and Transferability of the submitted Code\n",
    "1.  The Documentation of the submitted Code\n",
    "1.  Optional (not scored): Explanation of the Model's learned Relationships (e.g. through the Feature Importances)\n",
    "\n",
    "General Machine Learning Project Checklist (**Focus of this Challenge**) by [Aurélien Géron](https://github.com/ageron/handson-ml)\n",
    "\n",
    "1. Frame the Problem and look at the Big Picture\n",
    "1. Get the Data\n",
    "1. Explore the Data to gain Insights\n",
    "1. **Prepare the Data to better expose the underlying Data Patterns to the used Machine Learning Algorithms**\n",
    "1. Explore many different Models and short-list the best ones\n",
    "1. Fine-tune your Models and combine them into a great Solution\n",
    "1. Present your Solution\n",
    "1. Launch, monitor, and maintain your Model/Service\n",
    "\n",
    "INFO: Instead of working with [Google Colab](https://colab.research.google.com/), which is recommended because you can get started right away, or [Databricks](https://adb-7072220306909809.9.azuredatabricks.net/?o=7072220306909809), which is recommended if you want to collaborate in real-time, you can also work with your own Development Environment (e.g. [Visual Studio Code](https://code.visualstudio.com/)), by using [Git](https://git-scm.com/) to clone the [verovis Coding Challenge GitHub Repository](https://github.com/Jonas-Metz-verovis/verovis_Coding_Challenge) and collaborate e.g. by using [Microsoft Visual Studio Live Share](https://marketplace.visualstudio.com/items?itemName=MS-vsliveshare.vsliveshare-pack)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## A Short Notice:\n",
    "\n",
    "Depending on how many possible feature combinations you have found, it is sometimes not easy to keep track of which combination gains the most value for your model. Therefore, it is very common to use Scikit-Learn Pipelines or custom column transformers to have an easier understanding of what is happening. \n",
    "\n",
    "A short note about the goal of feature engineering. Feature engineering is perhaps the most essential part of machine learning, making the difference between a \"good\" and an \"excellent\" model. The goal of feature engineering is to increase the model's power. The power of a model depends on the given features. So, feature engineering e.g. tries to find valuable feature combinations which increase the model's power.\n",
    "\n",
    "To do feature engineering, follow these steps:\n",
    "\n",
    "-   Brainstorm features.\n",
    "-   Create features.\n",
    "-   Check how the features work with the model.\n",
    "-   Start again until your model's power stagnates."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Documentation and Support\n",
    "\n",
    "#### The following Resources might be useful to complete this Challenge:\n",
    "\n",
    "1.  [Scikit-Learn (Chi-Squared)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "1.  [Scikit-Learn (ColumnTransformer)](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "1.  [Scikit-Learn (Pipelines)](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "1.  [Medium: Scikit-Learn Pipelines](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)\n",
    "\n",
    "#### Further, more general Resources:\n",
    "\n",
    "1.  [Pandas Documentation](https://pandas.pydata.org/docs/reference/index.html#api)\n",
    "1.  [Numpy Documentation](https://numpy.org/doc/stable/)\n",
    "1.  [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/classes.html)\n",
    "1.  [Category Encoders Documentation](https://contrib.scikit-learn.org/category_encoders/)\n",
    "1.  [Imbalanced-Learn Documentation](https://imbalanced-learn.readthedocs.io/en/stable/api.html)\n",
    "1.  [Seaborn Documentation](https://seaborn.pydata.org/api.html)\n",
    "1.  [SHAP Documentation](https://shap.readthedocs.io/en/latest/api.html)\n",
    "1.  [Pandas Data Wrangling Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "1.  [TowardsDataScience: Data Cleansing](https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d)\n",
    "1.  [TowardsDataScience: Data Preprocessing](https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825)\n",
    "1.  [TowardsDataScience: Feature Engineering](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)\n",
    "1.  [Machine Learning Mastery: Feature Engineering](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "1.  [TowardsDataScience: Working with Numerical Variables](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "1.  [TowardsDataScience: Working with Categorical Variables](https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63)\n",
    "1.  [TowardsDataScience: Categorical Variable Encoding](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)\n",
    "1.  [TowardsDataScience: One-Hot-Encoding for tree-based Models](https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769)\n",
    "1.  [Stat Trek: One-Hot-Encoding (Dummy Variables)](https://stattrek.com/multiple-regression/dummy-variables.aspx)\n",
    "\n",
    "#### If you don't know how to find a Solution to a given Problem, it often works well if one just \"googles the problem\". Great Sources are:\n",
    "\n",
    "1.  [TowardsDataScience](https://towardsdatascience.com/)\n",
    "1.  [StackOverflow](https://stackoverflow.com/)\n",
    "1.  [Machine Learning Mastery](https://machinelearningmastery.com/start-here/)\n",
    "1.  [Python-Kurs.eu](https://www.python-kurs.eu/python3_kurs.php)\n",
    "1.  [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook)\n",
    "1.  [The Hitchhiker's Guide to Python](https://docs.python-guide.org/)\n",
    "1.  [Overview of Data Science YouTube Channels](https://towardsdatascience.com/top-20-youtube-channels-for-data-science-in-2020-2ef4fb0d3d5)\n",
    "1.  [Introduction to Machine Learning with Python](https://github.com/amueller/introduction_to_ml_with_python) / [Buy the Book](https://www.amazon.de/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)\n",
    "1.  [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf)\n",
    "1.  [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf)\n",
    "1.  [Deep Learning](https://www.deeplearningbook.org/)\n",
    "\n",
    "#### This Challenge was created by [Tim Fritzsche](tfritzsche@verovis.com) and [Jonas Metz](jmetz@verovis.com), please contact us anytime, if you have any Questions! :-)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Global Flags and Variables\n",
    "Please use the given RANDOM_STATE for all your Models etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_NAME = \"AdminTeam\"\n",
    "RANDOM_STATE = 42\n",
    "DATABRICKS_INSTANCE = \"https://adb-7072220306909809.9.azuredatabricks.net\"\n",
    "DATABRICKS_ORGANISATION = \"7072220306909809\""
   ]
  },
  {
   "source": [
    "# Databricks Specifics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm (\"/FileStore/\" + TEAM_NAME, recurse=True)\n",
    "dbutils.fs.mkdirs(\"/FileStore/\" + TEAM_NAME)\n",
    "dbutils.fs.ls(\"/FileStore/\" + TEAM_NAME)"
   ]
  },
  {
   "source": [
    "# Imports\n",
    "\n",
    "### Info (Google Colab)\n",
    "\n",
    "If you are working in Google Colab, you can install necessary (and not already installed) Packages by running e.g.\n",
    "\n",
    "```\n",
    "!pip install shap\n",
    "```\n",
    "\n",
    "### Info (Databricks)\n",
    "\n",
    "If you are working in [Databricks](https://docs.databricks.com/libraries/notebooks-python-libraries.html), you can install necessary (and not already installed) Packages by running e.g. this Command in the first Cell of your Notebook (the Kernel will reset after the Package has been installed):\n",
    "\n",
    "```\n",
    "%pip install shap\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from joblib import dump, load\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "source": [
    "# Data Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task: Load the Data from this CSV-File into a Pandas DataFrame\n",
    "\n",
    "Preprocessed_Data.csv: https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/Preprocessed_Data.csv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_file_link = \"https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/Preprocessed_Data.csv\"\n",
    "data = pd.read_csv (preprocessed_data_file_link, sep = \";\")"
   ]
  },
  {
   "source": [
    "# Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task: Brainstorm new Features\n",
    "The first task is to brainstorm. So, try to find some combination of the given features which may increase the power of the model. Note: Just write down your possible combinations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Add your possible Combinations here ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task: Create new Features\n",
    "The next task will be to create your \"brainstormed\" new features. Depending on how many new features you've found, it can be exhausting to evaluate them separately. Thus, we will use a custom column adder that gives us the advantage to be able to compare multiple feature combinations.\n",
    "\n",
    "The following class \"CombineNewFeatures\" was already created for you. Additionally, we have already added some feature combinations you may try. Create your new features and paste them into the given class.\n",
    "\n",
    "Note: You don't need the class to evaluate your new features, but it makes switching between different combinations of features a lot easier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineNewFeatures(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, add_smoker_satisfaction=False):\n",
    "        self.add_smoker_satisfaction = add_smoker_satisfaction\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.add_smoker_satisfaction:\n",
    "            X['Smoker_Satisfaction'] = 0\n",
    "            X.loc [((X [\"Smoking_Allowed\"] == 1) & (X [\"Smoker\"] == 1)) | ((X [\"Smoking_Allowed\"] == 0) & (X [\"Smoker\"] == 0)), \"Smoker_Satisfaction\"] = 1\n",
    "            X.loc [((X [\"Smoking_Allowed\"] == 1) & (X [\"Smoker\"] == 0)) | ((X [\"Smoking_Allowed\"] == 0) & (X [\"Smoker\"] == 1)), \"Smoker_Satisfaction\"] = -1\n",
    "            # Drop altered feature from input DataFrame\n",
    "            X.drop(['Smoker', 'Smoking_Allowed'], axis=1, inplace=True)\n",
    "            # Encode Smoker_Satisfaction as category\n",
    "            cat_smoker = CategoricalDtype(categories=[-1,0,1], ordered=True)\n",
    "            X['Smoker_Satisfaction'] = X['Smoker_Satisfaction'].astype(cat_smoker)\n",
    "        else:\n",
    "            X[['Smoker','Smoking_Allowed']] = X[['Smoker','Smoking_Allowed']].astype(int)\n",
    "        \n",
    "        # Add new custom Transformations here ...\n",
    "\n",
    "        # Encode all unmodified Features\n",
    "        cat_ratings = CategoricalDtype(categories=[0,1,2], ordered=True)\n",
    "        cat_distance= CategoricalDtype(categories=[-1,0,1], ordered=True)\n",
    "        \n",
    "        X['Rating'] = X['Rating'].astype(cat_ratings)\n",
    "        X['Distances_Category'] = X['Distances_Category'].astype(cat_distance)\n",
    "        # Add other unmodified Features here ...\n",
    "\n",
    "        # Drop placeID, userID\n",
    "        X = X.drop(['userID', 'placeID'], axis=1)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "source": [
    "## Task: Initiate Object\n",
    "Note: You only need to complete this task if you want to use the custom \"CombineNewFeatures\" Class\n",
    "\n",
    "Create an object 'add_feature' of the class \"CombineNewFeatures\". The object will be very handy if you just want to use specific features for your model. The default settings of using a new feature are set to False. Thus, if you want to use a new feature set the attribute to True and initiate the new object.\n",
    "\n",
    "To use the transform function, run:\n",
    "\n",
    "add_feature.transform(<your_data>)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "#add_feature = CombineNewFeatures(add_smoker_satisfaction=True)"
   ]
  },
  {
   "source": [
    "## Task: Feature Types\n",
    "In the next task, you need to ensure that your newly created features have the right type-encoding regarding their scale level. If you are confident that everything is right, store your feature names in a list. Make sure that just those features are included which you want to use for this model run."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_features_nominal = ...\n",
    "# input_features_ordinal = ...\n",
    "# target = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Input and Output DataFrames\n",
    "# X = ...\n",
    "# y = ..."
   ]
  },
  {
   "source": [
    "## Task: Transformer\n",
    "The next task is to use Scikit-Learn's \"ColumnTransformer\". To use the \"ColumTransfomer\" correctly, you need to know what kind of transformation the individual features will need. Use the following code to add or remove column transformations.\n",
    "\n",
    "Note: The \"ColumnTransformer\" always returns a Numpy Array"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ColumnTransformer and transform only the input data (X)\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Add Transformers here ...\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "X_transformed = transformer.fit_transform(X)"
   ]
  },
  {
   "source": [
    "## Task: Train-Test-Split\n",
    "Split the data as you've have already learned in the first coding challenge and fit your your model to the training data.\n",
    "\n",
    "Note: Use the train_test_split() Function from Scikit-Learn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "source": [
    "## Task: Summarize and explain your Feature Selection/Engineering Choices.\n",
    "\n",
    "#### Discuss/answer the following Questions:\n",
    "-   Which Features did you select?\n",
    "-   How did you encode the selected Categorical Variables and why are your Encoding Methods an appropriate Choice for these Features?\n",
    "-   Which Assumptions did you make?\n",
    "-   Which Hypotheses did you formulate about the Relationship between the selected Features and the Target Variables?\n",
    "-   What possible Limitations do you expect and what are the Reasons for your Expectation?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Model Calculation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task: Scikit-Learn Pipelines\n",
    "\n",
    "### Create a Pipeline which contains all necessary Transformations as well as a tree-based Model (e.g. Decision Tree/Random Forest) to predict the Rating of a Restaurant. Calculate the Accuracy Score of the Model (Training and Test Data). The Test Score will be used to grade this Challenge.\n",
    "\n",
    "Note: In this task, you should use Scikit-Learn's Pipeline Functionalities. A Pipeline can include multiple steps like a custom feature adder, column transformations and the model itself. Your goal is now to choose a model and define the Pipeline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Pipeline and Model Training Code here ..."
   ]
  },
  {
   "source": [
    "## Solution: The Test Score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print your Test Score here ..."
   ]
  },
  {
   "source": [
    "### Task (Optional): Optimize your Feature Selection Workflow as well as the Hyperparameters of your tree-based Model. To do this properly, you need to split your Data into a Training/Validation/Test Dataset (e.g. 60/20/20 %). Train your Models using the Training Dataset. Select the best model based on its Score on the Validation Dataset. Only score the (final) best Model on the Test Dataset to create a true out-of-sample Test Score. You can, of course, use Scikit-Learn's Pipelines/Grid Search/Cross Validation/etc. Functionalities if you like."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Feel free to be creative ;-)\")"
   ]
  },
  {
   "source": [
    "# Result Postprocessing\n",
    "\n",
    "## Task: Predict the Ratings using your best model. Combine the Predictions and the Actuals to one DataFrame."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this to select the best Model you've created\n",
    "best_model = ...\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Create a Pandas DataFrame with the actual as well as the predicted Restaurant Ratings\n",
    "df_predictions = pd.DataFrame({\n",
    "    'Actuals': y_test,\n",
    "    'Predictions': predictions})\n",
    "\n",
    "df_predictions.head()"
   ]
  },
  {
   "source": [
    "# Data Saving\n",
    "\n",
    "### Info (Google Colab)\n",
    "\n",
    "If you are working in Google Colab, you can save the Results to your Google Drive by running\n",
    "\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "\n",
    "You will be requested to authenticate with your Google Account.\n",
    "\n",
    "The Path to your Google Colab Notebooks Folder will be \"/content/drive/My Drive/Colab Notebooks\".\n",
    "\n",
    "The Commands can then use this Path:\n",
    "\n",
    "```\n",
    "os.makedirs (\"/content/drive/My Drive/Colab Notebooks/Results\", exist_ok=True)\n",
    "df_predictions.to_csv (\"/content/drive/My Drive/Colab Notebooks/Results/Restaurant_Rating_Predictions.csv\", index=False)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task: Save a DataFrame which contains the actual Test Ratings as well as the corresponding Test Predictions to a CSV-File.\n",
    "Please write the CSV-File in a way which can be read by a German Microsoft Excel without any necessary Modifications and submit the CSV-File together with your Solution Notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.to_csv (os.path.join (\"/dbfs/FileStore/\" + TEAM_NAME, TEAM_NAME + \"_Restaurant_Rating_Predictions_CC2.csv\"), sep=\";\",\n",
    "                       decimal=\",\", header=True, index=False, encoding=\"utf-8\", float_format=\"%.4f\")\n",
    "print (\"The Predictions have been successfully saved to a CSV-File, you can download them from:\")\n",
    "print (DATABRICKS_INSTANCE + \"/files/\" + TEAM_NAME + \"/\" + TEAM_NAME + \"_Restaurant_Rating_Predictions_CC2.csv\" + \"?o=\" + DATABRICKS_ORGANISATION)"
   ]
  },
  {
   "source": [
    "## Task: Save your Model to a joblib Pickle Dump File, which can be loaded during Inference. Please submit this File together with your Solution Notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = TEAM_NAME + \"_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_Coding_Challenge_02.joblib\"\n",
    "dump(best_model, os.path.join(\"/dbfs/FileStore/\" + TEAM_NAME, model_name))\n",
    "print (\"The fitted Model has been successfully saved, you can download it from:\")\n",
    "print (DATABRICKS_INSTANCE + \"/files/\" + TEAM_NAME + \"/\" + model_name + \"?o=\" + DATABRICKS_ORGANISATION)"
   ]
  }
 ]
}