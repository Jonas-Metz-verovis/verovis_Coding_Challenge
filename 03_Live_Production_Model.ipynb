{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/Jonas-Metz-verovis/verovis_Coding_Challenge/blob/main/03_Live_Production_Model.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introduction - Coding Challenge #3 - Live Production Model\n",
    "\n",
    "**Today's Coding Challenge focuses on a Live Production Model. The underlying case is based on data from a Portuguese bank. The data contains information from a telephonic marketing campaign, i.e. information about potential customers who have (not) subscribed to a term deposit after the phone call. Telephonic marketing campaigns still remain one of the most effective ways to reach out to people. However, they require massive investment as large call centers usually must be hired to execute these campaigns. Hence, it is crucial to identify the customers, which will be most likely to convert, before starting the campaign, so that they can be targeted explicitly.**\n",
    "\n",
    "\n",
    "**This challenge's main task is to deliver a smooth-running model ready to be used in a live production environment. Therefore, you'll have access to two different datasets. The first dataset (Marketing.csv) contains the so-called raw data. For the second dataset (Marketing_Live.csv), we'll assume that it contains the unknown data on which the model will be used in the live production environment. The big challenge is to deliver a full pipeline, capable of performing excellent on your training data and also usable in the live environment.**\n",
    "\n",
    "**The Challenge will be scored based on:**\n",
    "\n",
    "1.  The Prediction Model's Test Accuracy Score\n",
    "1.  The verbal Explanations for specific Processing/Modeling Choices\n",
    "1.  The Readability and Transferability of the submitted Code\n",
    "1.  The Documentation of the submitted Code\n",
    "1.  Optional (not scored): Explanation of the Model's learned Relationships (e.g. through the Feature Importances)\n",
    "\n",
    "General Machine Learning Project Checklist (**Focus of this Challenge**) by [Aurélien Géron](https://github.com/ageron/handson-ml)\n",
    "\n",
    "1. Frame the Problem and look at the Big Picture\n",
    "1. Get the Data\n",
    "1. Explore the Data to gain Insights\n",
    "1. Prepare the Data to better expose the underlying Data Patterns to the used Machine Learning Algorithms\n",
    "1. **Explore many different Models and short-list the best ones**\n",
    "1. Fine-tune your Models and combine them into a great Solution\n",
    "1. Present your Solution\n",
    "1. **Launch, monitor, and maintain your Model/Service**\n",
    "\n",
    "**INFO:** Instead of working with [Google Colab](https://colab.research.google.com/), which is recommended because you can get started right away, or [Databricks](https://adb-7072220306909809.9.azuredatabricks.net/?o=7072220306909809), which is recommended if you want to collaborate in real-time, you can also work with your own Development Environment (e.g. [Visual Studio Code](https://code.visualstudio.com/)), by using [Git](https://git-scm.com/) to clone the [verovis Coding Challenge GitHub Repository](https://github.com/Jonas-Metz-verovis/verovis_Coding_Challenge) and collaborate e.g. by using [Microsoft Visual Studio Live Share](https://marketplace.visualstudio.com/items?itemName=MS-vsliveshare.vsliveshare-pack)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Documentation and Support\n",
    "\n",
    "#### The following Resources might be useful to complete this Challenge:\n",
    "\n",
    "1.  [Scikit-Learn (Chi-square)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "1.  [Scikit-Learn (ColumnTransformer)](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "1.  [Scikit-Learn (Pipelines)](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "1.  [Medium: Scikit-Learn Pipelines](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)\n",
    "1.  Joblib [dump](https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html) and [load](https://joblib.readthedocs.io/en/latest/generated/joblib.load.html) Documentation\n",
    "\n",
    "<hr>\n",
    "\n",
    "1.  [Pandas Documentation](https://pandas.pydata.org/docs/reference/index.html#api)\n",
    "1.  [Numpy Documentation](https://numpy.org/doc/stable/)\n",
    "1.  [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/classes.html)\n",
    "1.  [Category Encoders Documentation](https://contrib.scikit-learn.org/category_encoders/)\n",
    "1.  [Imbalanced-Learn Documentation](https://imbalanced-learn.readthedocs.io/en/stable/api.html)\n",
    "1.  [Seaborn Documentation](https://seaborn.pydata.org/api.html)\n",
    "1.  [SHAP Documentation](https://shap.readthedocs.io/en/latest/api.html)\n",
    "1.  [Pandas Data Wrangling Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "1.  [TowardsDataScience: Data Cleansing](https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d)\n",
    "1.  [TowardsDataScience: Data Preprocessing](https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825)\n",
    "1.  [TowardsDataScience: Feature Engineering](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)\n",
    "1.  [Machine Learning Mastery: Feature Engineering](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "1.  [TowardsDataScience: Working with Numerical Variables](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "1.  [TowardsDataScience: Working with Categorical Variables](https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63)\n",
    "1.  [TowardsDataScience: Categorical Variable Encoding](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)\n",
    "1.  [TowardsDataScience: One-Hot-Encoding for tree-based Models](https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769)\n",
    "1.  [Stat Trek: One-Hot-Encoding (Dummy Variables)](https://stattrek.com/multiple-regression/dummy-variables.aspx)\n",
    "\n",
    "#### If you don't know how to find a Solution to a given Problem, it often works well if one just \"googles the problem\". Great Sources are:\n",
    "\n",
    "1.  [TowardsDataScience](https://towardsdatascience.com/)\n",
    "1.  [StackOverflow](https://stackoverflow.com/)\n",
    "1.  [Machine Learning Mastery](https://machinelearningmastery.com/start-here/)\n",
    "1.  [Python-Kurs.eu](https://www.python-kurs.eu/python3_kurs.php)\n",
    "1.  [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook)\n",
    "1.  [The Hitchhiker's Guide to Python](https://docs.python-guide.org/)\n",
    "1.  [Overview of Data Science YouTube Channels](https://towardsdatascience.com/top-20-youtube-channels-for-data-science-in-2020-2ef4fb0d3d5)\n",
    "1.  [Introduction to Machine Learning with Python](https://github.com/amueller/introduction_to_ml_with_python) / [Buy the Book](https://www.amazon.de/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)\n",
    "1.  [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf)\n",
    "1.  [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf)\n",
    "1.  [Deep Learning](https://www.deeplearningbook.org/)\n",
    "1.  [Hyndman/Athanasopoulos, Forecasting: Principles and Practice](https://otexts.com/fpp2/)\n",
    "\n",
    "#### This Challenge was created by [Tim Fritzsche](tfritzsche@verovis.com), [Jonas Metz](jmetz@verovis.com) and [Marcel Fynn Froboese](mfroboese@verovis.com), please contact us anytime, if you have any Questions! :-)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Global Flags and Variables\n",
    "Please use the given RANDOM_STATE for all your Models etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# TODO: Please choose a Team Name!\n",
    "TEAM_NAME = 'AdminTeam'\n",
    "\n",
    "DATABRICKS_INSTANCE = \"https://adb-7072220306909809.9.azuredatabricks.net\"\n",
    "DATABRICKS_ORGANISATION = \"7072220306909809\"\n",
    "DATABRICKS_BASE_DIRECTORY = os.path.join (\"/dbfs/FileStore\", TEAM_NAME)\n",
    "\n",
    "MODELS = os.path.join (DATABRICKS_BASE_DIRECTORY, \"Models\")\n",
    "\n",
    "SAVE_MODEL = True\n",
    "SAVE_PIPELINE = True"
   ]
  },
  {
   "source": [
    "# Databricks Specifics\n",
    "\n",
    "[Databricks Filestore Documentation](https://docs.databricks.com/data/filestore.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm (\"/FileStore/\" + TEAM_NAME, recurse=True)\n",
    "dbutils.fs.mkdirs(\"/FileStore/\" + TEAM_NAME + \"/Models\")\n",
    "dbutils.fs.ls(\"/FileStore/\" + TEAM_NAME)"
   ]
  },
  {
   "source": [
    "# Imports\n",
    "\n",
    "### Info (Google Colab)\n",
    "\n",
    "If you are working in Google Colab, you can install necessary (and not already installed) Packages by running e.g.\n",
    "\n",
    "```\n",
    "!pip install shap\n",
    "```\n",
    "\n",
    "### Info (Databricks)\n",
    "\n",
    "If you are working in [Databricks](https://docs.databricks.com/libraries/notebooks-python-libraries.html), you can install necessary (and not already installed) Packages by running e.g. this Command in the first Cell of your Notebook (the Kernel will reset after the Package has been installed):\n",
    "\n",
    "```\n",
    "%pip install shap\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import calendar\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import dump, load\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from collections import Counter\n",
    "from datetime import date, datetime\n",
    "from numpy.lib.npyio import zipfile_factory\n",
    "from numpy.ma.core import make_mask\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm.notebook import tqdm\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix, plot_roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "source": [
    "# Helper Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(numeric_feat=None, ordinal_feat=None, nominal_feat=None):\n",
    "    \"\"\"\n",
    "    Changes the type properly and directly\n",
    "    \n",
    "    \"\"\"\n",
    "    for num in numeric_feat:\n",
    "        Marketing[num] = Marketing[num].astype('int32')\n",
    "\n",
    "    for ord in ordinal_feat:\n",
    "        Marketing[ord] = Marketing[ord].astype('category')\n",
    "\n",
    "    for nom in nominal_feat:\n",
    "        Marketing[nom] = Marketing[nom].astype('object')\n",
    "\n",
    "\n",
    "def get_features_type(df):\n",
    "    \"\"\"\n",
    "    Sort the Features according to its Level and returns the names of the columns\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "    df: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    nominal_feat: list\n",
    "\n",
    "    ordinal_feat: list\n",
    "\n",
    "    numeric_feat: list\n",
    "    \n",
    "    \"\"\"\n",
    "    nominal_feat = [feat for feat in df.columns if df[feat].dtypes == 'object']\n",
    "    ordinal_feat = df.select_dtypes(include=['category']).columns.tolist()\n",
    "    numeric_feat = [feat for feat in df.columns if df[feat].dtypes == 'int32']\n",
    "    passed_feat = [feat for feat in df.columns if df[feat].dtypes == 'float64']\n",
    "    return nominal_feat, ordinal_feat, numeric_feat, passed_feat\n",
    "\n",
    "\n",
    "def get_feat_imp(model_metrics:dict):\n",
    "    \"\"\"\n",
    "    This function returns the feature importances with respect to the used transformation and the used estimator\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    transformation_names = {}\n",
    "    for key in model_metrics['DecisionTreeClassifier']['Best_Model (GridSearch)']['columntransformer'].transformers_:\n",
    "        if str(key[1]) == 'OneHotEncoder()':\n",
    "            transformation_names[key[0]] = key[1].get_feature_names(input_features=key[-1]).tolist()\n",
    "        # print(key[0], key[-1])\n",
    "        else:\n",
    "            transformation_names[key[0]] = key[-1] \n",
    "\n",
    "    complete_feature_names = []\n",
    "    for value in transformation_names.values():\n",
    "        complete_feature_names.append(value)\n",
    "\n",
    "    complete_feature_names = list(itertools.chain(*complete_feature_names))\n",
    "\n",
    "    feature_importances_dict = {'Feature Names': complete_feature_names}\n",
    "    for key in model_metrics.keys():\n",
    "        feature_importances_dict[key + '_Import.'] = model_metrics[str(key)]['Best_Model (GridSearch)'][str(key).lower()].feature_importances_.tolist()\n",
    "\n",
    "    result = pd.DataFrame(feature_importances_dict)\n",
    "\n",
    "    return result.sort_values(by='DecisionTreeClassifier_Import.', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "source": [
    "# Data Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Marketing_File_Link = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/Marketing/Marketing.csv'\n",
    "Marketing_Live_File_Link = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/Marketing/Marketing_Live.csv'\n",
    "\n",
    "Marketing = pd.read_csv(Marketing_File_Link, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Marketing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_feat = ['marital', 'default','housing','loan','contact','month', 'job','education']\n",
    "ordinal_feat = ['poutcome'] \n",
    "numeric_feat = ['age', 'balance', 'day', 'duration', 'campaign','pdays','previous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Types of Features\n",
    "change_dtypes(numeric_feat, ordinal_feat, nominal_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent Target-Leakage and choose column \"y\" as target\n",
    "target = Marketing['y']\n",
    "Marketing = Marketing.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Marketing, target, test_size=0.33, random_state=RANDOM_STATE, shuffle=False)"
   ]
  },
  {
   "source": [
    "## Show Feature Distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in X_train.columns:\n",
    "#     sns.displot(X_train[col])\n",
    "#     plt.xticks(rotation=45)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=4, nrows=2, figsize=(20, 8))\n",
    "\n",
    "for feat, ax in zip(nominal_feat, axes.flatten()):\n",
    "    sns.countplot(X_train[feat], ax=ax)\n",
    "    plt.suptitle('Nominal - Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, nrows=2, figsize=(20, 8))\n",
    "for feat, ax in zip(numeric_feat, axes.flatten()):\n",
    "    sns.distplot(X_train[feat], ax=ax)\n",
    "plt.tight_layout()\n",
    "\n",
    "#display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.countplot(X_train['poutcome'], ax=axes[0])\n",
    "sns.countplot(y_train, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "\n",
    "#display (fig)"
   ]
  },
  {
   "source": [
    "# Feature Engineering\n",
    "\n",
    "**Note:**\n",
    "You can use the additional feature engineering capabilities of the custom \"*CombineFeatureTransformer\"-Class*. The Class can be added to a *Scikit-Learn Pipeline* and can be stored as part of the pipeline.\n",
    "\n",
    "## Short Summary\n",
    "### Features (numerical):\n",
    "- **age:**\n",
    "The variable *age* represents the age of the person.\n",
    "\n",
    " - **balance:**\n",
    "The variable *balance* represents the average yearly balance, in euros.\n",
    "\n",
    "- **day:**\n",
    "The variable *day* represents the last contact day of the month.\n",
    "\n",
    "- **duration:**\n",
    "The variable *duration* represents the last contact duration.\n",
    "\n",
    "\n",
    "- **pdays:**\n",
    "The variable *pdays* represents the number of days that passed by after the client was last contacted from a previous campaign (-1 means client was not previously contacted)\n",
    "\n",
    "- **previous:** \n",
    "The variable *previous* represents the number of contacts performed before this campaign and for this client.\n",
    "\n",
    "- **campaign:**\n",
    "The variable *campaign* represents the number of contacts performed during this campaign and for this client.\n",
    "\n",
    "### Features (categorical)\n",
    "- **y:**\n",
    "The variable *y* represents the outcome of the campaign and therefore, if the client has subscribed a term deposit or not.\n",
    "\n",
    "- **marital:**\n",
    "The variable *marital* represents the marital-status of each person.\n",
    "\n",
    "- **default:**\n",
    "The variable *default* represents the presents of a credit default.\n",
    "\n",
    "- **housing:**\n",
    "The variable *housing* represents the presents of a housing loan.\n",
    "\n",
    "- **loan:**\n",
    "The variable *loan* represents the presents of a personal loan.\n",
    "\n",
    "- **contact**\n",
    "The variable *contact* represents the type of communication\n",
    "\n",
    "- **month:**\n",
    "The variable *month* represents the last contact month of the year.\n",
    "\n",
    "- **job:**\n",
    "The variable *job* represents the type of job.\n",
    "\n",
    "- **education:**\n",
    "The variable *education* represents the status of the education.\n",
    "\n",
    "- **poutcome:**\n",
    "The variable represents the outcome of the previous marketing campaign."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Task (optional): Feature Engineering is optional for this challenge. Of course, you can always improve the performance of your model if you wish.\n",
    "\n",
    "INFO: The CombineFeatureTransformer gives you the possibility to make additional transformations for specified features. The advantage of the CombineFeatureTransformer is the simple implementation into the pipeline itself. The pipeline contains all the necessary steps to prepare new unknown data for the corresponding model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,\n",
    "                 add_Benefit_Deposit=False,\n",
    "                 add_Affordable_Invest=False,\n",
    "                 add_New_Income=False,\n",
    "                 add_Contact_Before=False,\n",
    "                 add_Passed_Time=False,\n",
    "                 add_Risk_Protection=False,\n",
    "                 dropping_non_valuable_feat=False):\n",
    "        self.add_Benefit_Deposit = add_Benefit_Deposit\n",
    "        self.add_Affordable_Invest = add_Affordable_Invest\n",
    "        self.add_New_Income = add_New_Income\n",
    "        self.add_Contact_Before = add_Contact_Before\n",
    "        self.add_Passed_Time = add_Passed_Time\n",
    "        self.add_Risk_Protection = add_Risk_Protection\n",
    "\n",
    "        #Dropping non valuable features:\n",
    "        self.dropping_non_valuable_feat = dropping_non_valuable_feat\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Protect the original DataFrame\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        ##################################################################################################################################################\n",
    "        # Feel free to manipulate or add new features as you like\n",
    "        ##################################################################################################################################################\n",
    "        ##################################################################################################################################################\n",
    "        # Add your new Features here\n",
    "        ##################################################################################################################################################\n",
    "        \n",
    "        # if self.<YourFeatureName>:\n",
    "        #     <Your computation>\n",
    "\n",
    "\n",
    "\n",
    "        # Info: If your new feature is ready for the estimator (model), please typewrite your feature as 'float' since you want to use additional column transformation later.\n",
    "\n",
    "        ##################################################################################################################################################\n",
    "        # End of your Features\n",
    "        ##################################################################################################################################################\n",
    "\n",
    "        # To ensure the ordinal scale\n",
    "        ord_poutcome = pd.Categorical(X_copy['poutcome'], categories=['unknown','failure','other','success'], ordered=True)\n",
    "        labels_poutcome, unique_poutcome = pd.factorize(ord_poutcome, sort=True)\n",
    "        X_copy['poutcome'] = labels_poutcome\n",
    "        X_copy['poutcome'] = X_copy['poutcome'].astype('float')\n",
    "\n",
    "        ord_education = pd.Categorical(X_copy['education'], categories=['secondary','primary','unknown','tertiary'], ordered=True)\n",
    "        labels_education, unique_education = pd.factorize(ord_education, sort=True)\n",
    "        X_copy['education'] = labels_education\n",
    "        X_copy['education'] = X_copy['education'].astype('float')\n",
    "\n",
    "        ord_job = pd.Categorical(X_copy['job'], categories=['services','blue-collar','admin.','technician','student',\n",
    "                                                            'housemaid','entrepreneur','unemployed','self-employed','management','unknown','retired'], ordered=True)\n",
    "        labels_job, unique_job = pd.factorize(ord_job, sort=True)\n",
    "        X_copy['job'] = labels_job\n",
    "        X_copy['job'] = X_copy['job'].astype('float')\n",
    "\n",
    "        if self.add_Benefit_Deposit:        \n",
    "            # Benefits the most from term deposit\n",
    "            X_copy['Benefit_Deposit'] = pd.cut(X_copy['age'], 5, labels=[-2, -1 ,0, 1, 2])\n",
    "            X_copy['Benefit_Deposit'] = X_copy['Benefit_Deposit'].astype('float')\n",
    "\n",
    "        if self.add_Affordable_Invest:\n",
    "            # Term deposit possible if balance > 2000€\n",
    "            X_copy['Affordable_Invest'] = np.where(X_copy['balance'] <=2000, 0, 1)\n",
    "            X_copy['Affordable_Invest'] = X_copy['Affordable_Invest'].astype('float')\n",
    "\n",
    "        if self.add_New_Income:\n",
    "            # New Income is available for customer\n",
    "            conditions_new_income = [((X['day'] >= 28) & (X['day'] <= 31)) | ((X['day'] >= 12) & (X['day'] <= 16)) | ((X['day'] >= 1) & (X['day'] <= 3))]\n",
    "            X_copy['New Income'] = np.select(conditions_new_income, [1], default=0)\n",
    "            X_copy['New Income'] = X_copy['New Income'].astype('float')\n",
    "\n",
    "        if self.add_Contact_Before:\n",
    "            # Customer was contacted before\n",
    "            X_copy['Contact Before'] = np.where(X['pdays'] < 0, 0, 1)\n",
    "            X_copy['Contact Before'] = X_copy['Contact Before'].astype('float')\n",
    "\n",
    "        if self.add_Passed_Time:\n",
    "            # Time passed after the last campaign\n",
    "            X_copy['Passed Time'] = pd.cut(X['pdays'], bins=5, labels=[-2, -1, 0, 1, 2])\n",
    "            X_copy['Passed Time'] = X_copy['Passed Time'].astype('float')\n",
    "\n",
    "        if self.add_Risk_Protection:\n",
    "            # Customers have engough Risk protection based on marital and balance to invest in term deposit\n",
    "            conditions = [(X['marital'] == 'married') & (X['balance'] > 2000),\\\n",
    "                 (X['marital'] == 'single') & (X['balance'] > 2500),\\\n",
    "                      (X['marital'] == 'divorced') & (X['balance'] > 3000)]\n",
    "            choices = [1 ,1, 1]\n",
    "            X_copy['Risk Protection'] = np.select(conditions, choices, default=0)\n",
    "            X_copy['Risk Protection'] = X_copy['Risk Protection'].astype('float')\n",
    "\n",
    "        ##################################################################################################################################################\n",
    "        \n",
    "        if self.dropping_non_valuable_feat:\n",
    "            # Droping Fetures after Feature-Importances\n",
    "            droppings = ['month', 'poutcome', 'marital' ,'previous', 'default', 'loan', 'housing', 'pdays', 'day', 'job']\n",
    "            X_copy.drop(droppings, axis=1, inplace=True)\n",
    "        \n",
    "        return X_copy"
   ]
  },
  {
   "source": [
    "## Initialize the CombineFeatureTransformer\n",
    "### Task (optional): If you perform your feature engineering via the \"CombineFeatureTransformer-Approach\", you can easily handle feature selection here.\n",
    "\n",
    "**IMPORTANT: If you don't want to use the CombineFeatureTransformer, set all attributes to False. But make sure that the following code is still based on it.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Feature Transformer\n",
    "add_custom_features = CombineFeatureTransformer(add_Benefit_Deposit=False, add_Affordable_Invest=False, add_New_Income=False,\n",
    "                                                add_Contact_Before=False, add_Passed_Time=True, add_Risk_Protection=False, dropping_non_valuable_feat=True)\n",
    "X_train_temp = add_custom_features.transform(X_train)\n",
    "\n",
    "# INFO: The Pipeline's OrdinalEncoder or OneHotEncoder work only with a Pandas DataFrame if the feature names are properly stored. Thus, for additional adding or deleting of features it is neccessary to update the feature names. If the type of the features is correct, you can use the function \"get_features_type\".\n",
    "nominal_feat, ordinal_feat, numeric_feat, passed_feat = get_features_type(X_train_temp)\n",
    "\n",
    "# There feature 'duration' and 'campaign' need to be Min-Max-Scaled\n",
    "minmax_feat = [feat for feat in numeric_feat if feat in ['duration', 'campaign']]\n",
    "numeric_feat.remove(minmax_feat[0])\n",
    "numeric_feat.remove(minmax_feat[1])"
   ]
  },
  {
   "source": [
    "## Create ColumnTransformer\n",
    "### Task (optional): Depending on your exploratory data analysis, some features need to be transformed. Alternative transformation steps can be implemented or omitted here. The task is optional."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ColumTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), nominal_feat),\n",
    "        ('minmax', MinMaxScaler(), minmax_feat),\n",
    "        ('stdscaler', StandardScaler(), numeric_feat),\n",
    "        ('passed', 'passthrough', passed_feat)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")"
   ]
  },
  {
   "source": [
    "## Counter imbalanced Data with Over- and Under-Sampling\n",
    "### Task (optional): Within the exploratory data analysis, you might have noticed that the data is sometimes very unevenly distributed. One way to counteract such a distribution is to generate the data synthetically within the cross-validation. Since this task is optional, additional information concerning over- and under-sampling can be found in the following links:\n",
    "\n",
    "1.  [SMOTE for imbalanced Classification with Python](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n",
    "1.  [Imbalanced-Learn Documentation](https://imbalanced-learn.org/stable/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter imbalanced Data\n",
    "sampling_over = None\n",
    "sampling_under = None"
   ]
  },
  {
   "source": [
    "## Models\n",
    "### Task (optional): Since the main focus of this challenge lies on the live production pipeline, two classifiers are already given here. In addition, however, the list can be supplemented by other estimation methods."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of classifiers\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    '...',\n",
    "    '<Your Additonal Models>',\n",
    "    '...'\n",
    "]"
   ]
  },
  {
   "source": [
    "## GridSearch Parameter\n",
    "### Task (optional): In order to find the best parameter configuration of the respective estimation method, the GridSearch approach is followed here. Additional parameters of the individual methods can supplement the following dictionary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Parameter for GridSearch\n",
    "\n",
    "# For additional Hyperparamter-Tuning alter the dictionary accordingly\n",
    "# INFO: To tune the hyperparameters dynamically you need to specify the attribute according to its estimator, e.g. max_depth would be decisiontreeclassifier__max_depth\n",
    "\n",
    "parameter = {\n",
    "    'DecisionTreeClassifier':{\n",
    "        'decisiontreeclassifier__max_depth': [2, 6, 8, 10],\n",
    "        # 'decisontreeclassifier__..........'\n",
    "    },\n",
    "\n",
    "    'RandomForestClassifier':{\n",
    "        'randomforestclassifier__n_estimators': [2, 4, 8, 10],\n",
    "        # 'randomforestclassifier__............'\n",
    "\n",
    "    },\n",
    "    # '<Your Additional Models>':{\n",
    "\n",
    "    # }\n",
    " \n",
    "}"
   ]
  },
  {
   "source": [
    "## Training and Scoring of the Models\n",
    "### Task: In the following code, you have to store both, the individual models and their respective pipelines as well as the best (final) pipeline. Please insert your code at the specified positions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INFO: You can use this Code as Template for \"how to save my Model in Databricks (and download it afterwards\"\n",
    "\n",
    "model_name = TEAM_NAME + \"_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_CC_03.joblib\"\n",
    "dump(model, os.path.join(MODELS, model_name))\n",
    "print (\"The fitted Model has been successfully saved, you can download it from:\")\n",
    "print (DATABRICKS_INSTANCE + \"/files/\" + TEAM_NAME + \"/Models/\" + model_name + \"?o=\" + DATABRICKS_ORGANISATION)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "%%time\n",
    "# Create a Dictionary to Store metrics\n",
    "model_metrics = {}\n",
    "\n",
    "# NOTE: For some reason the SMOTE-Method only works with \"make_pipeline\".\n",
    "for classifier in classifiers:\n",
    "    pipe_imb = make_pipeline(\n",
    "        add_custom_features,\n",
    "        preprocessor,\n",
    "        sampling_over,\n",
    "        sampling_under,\n",
    "        classifier\n",
    "    )\n",
    "\n",
    "    # Store String of Classifier\n",
    "    clf_str = str(classifier).replace('()','')\n",
    "\n",
    "    # Evaluate Model with ROC-AUC and ReapetedStartifiedKFold and using GridSearch for best Parameter\n",
    "    cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=42)\n",
    "\n",
    "    # Create List for scorer\n",
    "    scorer = ['accuracy', 'balanced_accuracy', 'roc_auc']\n",
    "\n",
    "    # GridSearch\n",
    "    grid_search = GridSearchCV(pipe_imb, param_grid=parameter.get(clf_str), scoring=scorer, n_jobs=-1, cv=cv, refit='roc_auc', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit grid Search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    #######################################################################################################################################################\n",
    "    # Task: Save each estimator with best performance according to its hyperparameters\n",
    "    #######################################################################################################################################################\n",
    "    if SAVE_MODEL:\n",
    "        # TODO: Add your code here ...\n",
    "\n",
    "\n",
    "    #######################################################################################################################################################\n",
    "    # Task: Save each pipeline\n",
    "    #######################################################################################################################################################\n",
    "    if SAVE_PIPELINE:\n",
    "        # TODO: Add your code here ...\n",
    "        \n",
    "\n",
    "    # Save to metrics\n",
    "    model_metrics[clf_str] = {'Accuracy': grid_search.cv_results_['mean_test_accuracy'].mean(),\n",
    "                              'Acc_Std': grid_search.cv_results_['std_test_accuracy'].mean(),\n",
    "                              'Balanced_Accuracy': grid_search.cv_results_['mean_test_balanced_accuracy'].mean(),\n",
    "                              'Balanced_Accuracy_Std': grid_search.cv_results_['std_test_balanced_accuracy'].mean(),\n",
    "                              'ROC_AUC': grid_search.cv_results_['mean_test_roc_auc'].mean(),\n",
    "                              'ROC_AUC_Std': grid_search.cv_results_['std_test_roc_auc'].mean(),\n",
    "                              'Best_Score (Grid Search)': grid_search.best_score_,\n",
    "                              'Best_Params (GridSearch)': grid_search.best_params_,\n",
    "                              'Best_Model (GridSearch)': grid_search.best_estimator_,\n",
    "    }\n",
    "    \n",
    "# Save metrics to dataframe\n",
    "model_metrics_df = pd.DataFrame(model_metrics).transpose()\n",
    "\n",
    "# Choose the best model(pipeline) \n",
    "best_pipeline = model_metrics_df[model_metrics_df['Best_Score (Grid Search)'] == model_metrics_df['Best_Score (Grid Search)'].max()]['Best_Model (GridSearch)'][0]\n",
    "best_pipeline_name = model_metrics_df[model_metrics_df['Best_Score (Grid Search)'] == model_metrics_df['Best_Score (Grid Search)'].max()].index[0]\n",
    "\n",
    "\n",
    "#######################################################################################################################################################\n",
    "# Task: Save the best (final) pipeline\n",
    "#######################################################################################################################################################\n",
    "if SAVE_PIPELINE:\n",
    "    # TODO: Add your code here ...\n",
    "        \n"
   ]
  },
  {
   "source": [
    "## Evaluate the Models\n",
    "### Task: Look at the results. What do you notice?\n",
    "\n",
    "INFO:If you want to get more detailed information use 'detailed_Overview'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics from used Models\n",
    "model_metrics_df.transpose()"
   ]
  },
  {
   "source": [
    "### Please insert your answer here:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed_Overview = pd.DataFrame(grid_search.cv_results_)\n",
    "# detailed_Overview"
   ]
  },
  {
   "source": [
    "### Task: Next, let's visualize and analyze the performance. What do you notice in general? What was the actual goal of this prediction model? Does the model work as necessary? Briefly write down what your observations:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Feature Importance\n",
    "# get_feat_imp(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion-matrix\n",
    "fig, axes = plt.subplots(ncols=len(classifiers), figsize=(20, 8))\n",
    "for title, pipe, ax in zip(classifiers, model_metrics_df['Best_Model (GridSearch)'], axes.flatten()):\n",
    "    conf_mat_disp = plot_confusion_matrix(pipe, X_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax)\n",
    "    conf_mat_disp.ax_.set_title(title)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=len(classifiers), figsize=(20, 8))\n",
    "for title, pipe, ax in zip(classifiers, model_metrics_df['Best_Model (GridSearch)'], axes.flatten()):\n",
    "    roc_auc_disp = plot_roc_curve(pipe, X_test, y_test, ax=ax)\n",
    "    roc_auc_disp.ax_.set_title(title)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "source": [
    "### Please insert your answer here:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Load Pipeline and predict on live Marketing Data\n",
    "### Task: Now imagine you've transferred your pipeline to the production environment. The next task is to reload the pipeline which you've just saved. Next, please load the unknown data (Marketing_Live.csv). To determine your prediction quality on the live data set, you can additionally load the True Values (Marketing_Live_Actuals.csv)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best Pipeline\n",
    "# Best_Pipeline = load('...')\n",
    "\n",
    "# # Load unknow data (Marketing_Live) and Actuals\n",
    "Marketing_Live = pd.read_csv(Marketing_Live_File_Link, sep=';')\n",
    "#Marketing_Live_Actuals_File_Link = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/Marketing/Marketing_Live_Actuals.csv'\n",
    "#y_actuals = pd.read_csv(Marketing_Live_Actuals_File_Link, sep=';')"
   ]
  },
  {
   "source": [
    "### Task: Next, use your pipeline to create your predictions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictions\n",
    "# y_pred = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marketing_Live['Prediction'] = '...'\n",
    "# Marketing_Live.head()"
   ]
  },
  {
   "source": [
    "# Data Saving\n",
    "\n",
    "### Info (Google Colab)\n",
    "\n",
    "If you are working in Google Colab, you can save the Results to your Google Drive by running\n",
    "\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "\n",
    "You will be requested to authenticate with your Google Account.\n",
    "\n",
    "The Path to your Google Colab Notebooks Folder will be \"/content/drive/My Drive/Colab Notebooks\".\n",
    "\n",
    "The Commands can then use this Path:\n",
    "\n",
    "```\n",
    "os.makedirs (\"/content/drive/My Drive/Colab Notebooks/Results\", exist_ok=True)\n",
    "df_predictions.to_csv (\"/content/drive/My Drive/Colab Notebooks/Results/Marketing_Live_Predictions.csv\", index=False)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Task: Save a DataFrame which contains the actual Live Targets as well as the corresponding Live Predictions to a CSV-File.\n",
    "Please write the CSV-File in a way which can be read by a German Microsoft Excel without any necessary Modifications and submit the CSV-File together with your Solution Notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marketing_Live.to_csv (os.path.join (DATABRICKS_BASE_DIRECTORY, TEAM_NAME + \"_Marketing_Live_Predictions.csv\"), sep=\";\", decimal=\",\", header=True, index=False, encoding=\"utf-8\", float_format=\"%.4f\")\n",
    "# print (\"The Predictions have been successfully saved to a CSV-File, you can download them from:\")\n",
    "# print (DATABRICKS_INSTANCE + \"/files/\" + TEAM_NAME + \"/\" + TEAM_NAME + \"_Marketing_Live_Predictions.csv\" + \"?o=\" + DATABRICKS_ORGANISATION)"
   ]
  }
 ]
}