{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd09ca9ced1fbb87ca4ccc1db140debb9f236777ff88c014956bbce083d2759c455",
   "display_name": "Python 3.7.10 64-bit ('Tim': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/Jonas-Metz-verovis/verovis_Coding_Challenge/blob/main/04_Time_Series_Forecasting.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introduction - Coding Challenge #4 - Time Series Analysis & Forecasting \n",
    "\n",
    "**Today's coding challenge focuses on time series analysis and time series prediction. In the first part of the challenge, you will demonstrate your theoretical knowledge. In the following section, you have to train your model, evaluate it, and predict the subsequent 17 periods. The data set contains quarterly data of the earnings per share of the company Johnson&Johnson.\n",
    "\n",
    "The main goal of the coding challenge is to develop a model that can predict the subsequent 17 earnings per share of Johnson&Johnson.**\n",
    "\n",
    "**The Challenge will be scored based on:**\n",
    "\n",
    "1.  The Prediction Model's Test Mean Absolute Percentage (MAPE) Score\n",
    "1.  The verbal Explanations for specific Processing/Modeling Choices\n",
    "1.  The Readability and Transferability of the submitted Code\n",
    "1.  The Documentation of the submitted Code\n",
    "1.  Optional (not scored): Explanation of the Model's learned Relationships (e.g. through the Feature Importances)\n",
    "\n",
    "General Machine Learning Project Checklist (**Focus of this Challenge**) by [Aurélien Géron](https://github.com/ageron/handson-ml)\n",
    "\n",
    "1. Frame the Problem and look at the Big Picture\n",
    "1. Get the Data\n",
    "1. Explore the Data to gain Insights\n",
    "1. Prepare the Data to better expose the underlying Data Patterns to the used Machine Learning Algorithms\n",
    "1. **Explore many different Models and short-list the best ones**\n",
    "1. Fine-tune your Models and combine them into a great Solution\n",
    "1. Present your Solution\n",
    "1. **Launch, monitor, and maintain your Model/Service**\n",
    "\n",
    "**INFO:** Instead of working with [Google Colab](https://colab.research.google.com/), which is recommended because you can get started right away, or [Databricks](https://adb-7072220306909809.9.azuredatabricks.net/?o=7072220306909809), which is recommended if you want to collaborate in real-time, you can also work with your own Development Environment (e.g. [Visual Studio Code](https://code.visualstudio.com/)), by using [Git](https://git-scm.com/) to clone the [verovis Coding Challenge GitHub Repository](https://github.com/Jonas-Metz-verovis/verovis_Coding_Challenge) and collaborate e.g. by using [Microsoft Visual Studio Live Share](https://marketplace.visualstudio.com/items?itemName=MS-vsliveshare.vsliveshare-pack)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Documentation and Support\n",
    "\n",
    "#### The following Resources might be useful to complete this Challenge:\n",
    "\n",
    "1.  [AR-Process (statsmodels)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.ar_model.AutoReg.html#statsmodels.tsa.ar_model.AutoReg)\n",
    "1.  [Forecasting: Principles and Practice (AR-Process)](https://otexts.com/fpp2/AR.html)\n",
    "1.  [Forecasting: Principles and Practice (MA-Process)](https://otexts.com/fpp2/MA.html)\n",
    "1.  [ARIMA-Process (statsmodels)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html#statsmodels.tsa.arima.model.ARIMA)\n",
    "1.  [Forecasting: Principles and Practice (ARIMA-Process)](https://otexts.com/fpp2/arima.html)\n",
    "1.  [Simple Exponential Smoothing (statsmodels)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html#statsmodels.tsa.holtwinters.SimpleExpSmoothing)\n",
    "1.  [Forecasting: Principles and Practice (Simple Exponential Smoothing)](https://otexts.com/fpp2/ses.html)\n",
    "1.  [Holt-Winters (statsmodels)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.Holt.html#statsmodels.tsa.holtwinters.Holt)\n",
    "1.  [Forecasting: Principles and Practice (Holt-Winters)](https://otexts.com/fpp2/holt-winters.html)\n",
    "\n",
    "<hr>\n",
    "\n",
    "1.  [Pandas Documentation](https://pandas.pydata.org/docs/reference/index.html#api)\n",
    "1.  [Numpy Documentation](https://numpy.org/doc/stable/)\n",
    "1.  [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/classes.html)\n",
    "1.  [Seaborn Documentation](https://seaborn.pydata.org/api.html)\n",
    "1.  [SHAP Documentation](https://shap.readthedocs.io/en/latest/api.html)\n",
    "1.  [Pandas Data Wrangling Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "1.  [TowardsDataScience: Data Cleansing](https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d)\n",
    "\n",
    "#### If you don't know how to find a Solution to a given Problem, it often works well if one just \"googles the problem\". Great Sources are:\n",
    "\n",
    "1.  [TowardsDataScience](https://towardsdatascience.com/)\n",
    "1.  [StackOverflow](https://stackoverflow.com/)\n",
    "1.  [Machine Learning Mastery](https://machinelearningmastery.com/start-here/)\n",
    "1.  [Python-Kurs.eu](https://www.python-kurs.eu/python3_kurs.php)\n",
    "1.  [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook)\n",
    "1.  [The Hitchhiker's Guide to Python](https://docs.python-guide.org/)\n",
    "1.  [Overview of Data Science YouTube Channels](https://towardsdatascience.com/top-20-youtube-channels-for-data-science-in-2020-2ef4fb0d3d5)\n",
    "1.  [Introduction to Machine Learning with Python](https://github.com/amueller/introduction_to_ml_with_python) / [Buy the Book](https://www.amazon.de/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)\n",
    "1.  [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf)\n",
    "1.  [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf)\n",
    "1.  [Deep Learning](https://www.deeplearningbook.org/)\n",
    "1.  [Hyndman/Athanasopoulos, Forecasting: Principles and Practice](https://otexts.com/fpp2/)\n",
    "\n",
    "#### This Challenge was created by [Tim Fritzsche](tfritzsche@verovis.com), [Jonas Metz](jmetz@verovis.com) and [Marcel Fynn Froboese](mfroboese@verovis.com), please contact us anytime, if you have any Questions! :-)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Global Flags and Variables\n",
    "Please use the given RANDOM_STATE for all your Models etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Check the Notebook on DataBricks\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# # TODO: Please choose a Team Name!\n",
    "# TEAM_NAME = 'AdminTeam'\n",
    "\n",
    "# DATABRICKS_INSTANCE = \"https://adb-7072220306909809.9.azuredatabricks.net\"\n",
    "# DATABRICKS_ORGANISATION = \"7072220306909809\"\n",
    "# DATABRICKS_BASE_DIRECTORY = os.path.join (\"/dbfs/FileStore\", TEAM_NAME)\n",
    "\n",
    "# MODELS = os.path.join (DATABRICKS_BASE_DIRECTORY, \"Models\")\n",
    "\n",
    "# SAVE_MODEL = True\n",
    "# SAVE_PIPELINE = True"
   ]
  },
  {
   "source": [
    "# Databricks Specifics\n",
    "\n",
    "[Databricks Filestore Documentation](https://docs.databricks.com/data/filestore.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# TODO: Create the config.py file and get sure that it is accessable on DataBricks itself\n",
    "\n",
    "# dbutils.fs.rm (\"/FileStore/\" + TEAM_NAME, recurse=True)\n",
    "# dbutils.fs.mkdirs(\"/FileStore/\" + TEAM_NAME + \"/Models\")\n",
    "# dbutils.fs.ls(\"/FileStore/\" + TEAM_NAME)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Imports\n",
    "\n",
    "### Info (Google Colab)\n",
    "\n",
    "If you are working in Google Colab, you can install necessary (and not already installed) Packages by running e.g.\n",
    "\n",
    "```\n",
    "!pip install shap\n",
    "```\n",
    "\n",
    "### Info (Databricks)\n",
    "\n",
    "If you are working in [Databricks](https://docs.databricks.com/libraries/notebooks-python-libraries.html), you can install necessary (and not already installed) Packages by running e.g. this Command in the first Cell of your Notebook (the Kernel will reset after the Package has been installed):\n",
    "\n",
    "```\n",
    "%pip install shap\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os \n",
    "import config\n",
    "from config import Load_Data\n",
    "from config import Generate_Process\n",
    "from config import plot_process\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# TimeSeries\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.regression.linear_model import yule_walker\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import pacf, acf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 14)\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "source": [
    "# Helper Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ARIMA(endog, ps=range, ds=int, qs=range, return_order=False):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters and corresponding AIC\n",
    "        and the best order direct.\n",
    "        \n",
    "        \n",
    "        endog:      the observed variable\n",
    "        ps:         Order range of parameter p\n",
    "        qs:         Order range of paramter q\n",
    "        ds:         Number of intergrations of the time series\n",
    "\n",
    "    \"\"\"\n",
    "    # Create all possible combination of the given order Parameter\n",
    "    parameters = product(ps, qs)\n",
    "    parameters_list = list(parameters)\n",
    "\n",
    "    # Store all possible combination in order_list\n",
    "    order_list = []\n",
    "    for each in parameters_list:\n",
    "        each = list(each)\n",
    "        each.insert(ds, 1)\n",
    "        each = tuple(each)\n",
    "        order_list.append(each)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # BUG: tqdm maybe don't work on Databricks\n",
    "    for order in tqdm_notebook(order_list):\n",
    "        try: \n",
    "            model = SARIMAX(endog, order=order, simple_differencing=False).fit(disp=False)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Choose AIC as decision metric\n",
    "        # TODO: Add one or two metrics\n",
    "        aic = model.aic\n",
    "        results.append([order, model.aic])\n",
    "        \n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.columns = ['(p, d, q)', 'AIC']\n",
    "    #Sort in ascending order, lower AIC is better\n",
    "    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    if return_order:\n",
    "        best_order = result_df.iloc[0]['(p, d, q)']\n",
    "        return result_df, best_order\n",
    "\n",
    "    else:\n",
    "        return result_df\n",
    "\n",
    "\n",
    "def metrics_dataframe(data, ps=range, ds=int, qs=range, n_splits=4):\n",
    "    \"\"\"\n",
    "        This function returns information about key-metrics calcualted on the respective Fold.\n",
    "    \"\"\"\n",
    "    metric_results = {}\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(data['values'])):\n",
    "        # get the train and test data according to the index\n",
    "        train = data['values'][train_index]\n",
    "        test = data['values'][test_index]\n",
    "\n",
    "        # Use the function optimize_ARIMA to define the best order of parameter on the specific train set. \n",
    "        # Then, use the best order (decision based on lowest AIC) to fit a temporarly model and calculate the respective metrics on the respective fold. \n",
    "        result_df, best_order = optimize_ARIMA(endog=train, ps=ps, ds=ds, qs=qs, return_order=True)\n",
    "        temp_model = SARIMAX(train, order=best_order, simple_differencing=False)\n",
    "        results = temp_model.fit()\n",
    "\n",
    "        # generate pseudo forecast to compare the actual values with forecast values. The length of the forecast must be the    length of the test! Then calculate the metrics based on the actual values and forecast values in test_set\n",
    "\n",
    "        temp_forecast = results.forecast(len(test))\n",
    "        #For checking if the date index is equal, we join on index and store the dataframes in metric_results\n",
    "        # dfs = pd.DataFrame(temp_forecast).join(pd.DataFrame(test))\n",
    "\n",
    "        metric_results['Fold_'+ str(fold)] = {\n",
    "            'MSE': mean_squared_error(y_true=test.values, y_pred=temp_forecast.values),\n",
    "            'RMSE': np.square(mean_squared_error(y_true=test.values, y_pred=temp_forecast.values)),\n",
    "            'MAE': results.mae,\n",
    "            'MAPE': mean_absolute_percentage_error(y_true=test.values, y_pred=temp_forecast.values),\n",
    "            'SSE': results.sse,\n",
    "            'AIC': results.aic,\n",
    "            'Date_Range_Train': (train.index[0], train.index[-1]),\n",
    "            'Date_Range_Test': (test.index[0], test.index[-1]),\n",
    "            'length_train': int(len(train_index)),\n",
    "            'length_test': int(len(test_index))\n",
    "            }\n",
    "\n",
    "    # Calculate the average over the folds:\n",
    "    df_metric = pd.DataFrame(metric_results)\n",
    "    df_metric['Fold_Mean'] = df_metric.loc[['MSE', 'RMSE', 'MAE', 'MAPE', 'SSE', 'AIC']].mean(axis=1)\n",
    "\n",
    "    return df_metric   \n",
    "\n"
   ]
  },
  {
   "source": [
    "# Data Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_ar_process = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/TimeSeries/Generated_Process/AR_PROCESS.csv'\n",
    "link_ma_process = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/TimeSeries/Generated_Process/MA_PROCESS.csv'\n",
    "link_arma_process = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/TimeSeries/Generated_Process/ARMA_PROCESS.csv'\n",
    "\n",
    "\n",
    "AR_PROCESS = pd.read_csv(link_ar_process, sep=';')\n",
    "MA_PROCESS = pd.read_csv(link_ma_process, sep=';')\n",
    "ARMA_PROCESS = pd.read_csv(link_arma_process, sep=';')"
   ]
  },
  {
   "source": [
    "# Time Series Analysis (Theory)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TASK I: \n",
    "Your first task is to visualize the AR- / MA- and ARMA-Process. Furthermore, try to determine the parameter order for an AR($p$), MA($q$) and an ARMA($p,q$)-Process.\n",
    "To answer this question please use the chunk below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your Code here !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your Code here !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your Code here !!!"
   ]
  },
  {
   "source": [
    "### (Task I) Answer :\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TASK II:\n",
    "You gained your first information to be able to forecast values of a time series. But not all information lay in the specific visualization. To increase the understanding how an *AR(p)* and a *MA(q)* works define in the following task the mathematical formula with repect to your findings in *Task I*.\n",
    "\n",
    "The formula for an *AR(p)* and *MA(q)* is given by:\n",
    "\n",
    "$$\\text{AR(p)} := \\quad y_t = c + \\sum_{i=1}^{p} \\ \\Phi_i \\ y_{t-i} + \\epsilon_t$$\n",
    "\n",
    "<br>\n",
    "and \n",
    "\n",
    "$$\\text{MA(q)} := \\quad y_t = \\mu + \\epsilon_t + \\Theta_1 \\epsilon_{t-1} + \\Theta_2 \\epsilon_{t-2} + ... + \\Theta_q \\epsilon_{t-q}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task II) Answer:\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task III:\n",
    "As you can see, AR(2)-Process takes the last two time points into account ($y_{t-1}$ and $y_{t-2}$) based on the order we assume. For the MA(2)-Process we take the passed observed error into account ($\\epsilon$). Both Process are depending on $\\Phi$ and $\\Theta$, respectively. The Question is now, how to determine the latter parameter. Obvisouly, this is a simple regression problem. The Task know is to determine both parameter with the help of the *statsmodels* Python package. \n",
    "\n",
    "<br>\n",
    "\n",
    "__Steps:__\n",
    "\n",
    "1.  Define your the *order*-Attribute (Sequence of the prameter is *p,d,q*)\n",
    "1. Fit the model and save it in the variable *AR_MODEL* and *MA_MODEL*, respectively.\n",
    "1. Print out your model summary with the method *<your_fitted_model>.summary()*.\n",
    "1. Use your mathematical formualtion from *Task II* and replace the corresponding parameter. \n",
    "\n",
    "__INFO: For now set your *d*-Parameter to null.__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task III) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling AR(2)-Process\n",
    "\n",
    "# 1. Define your order here: [order=(p, d, q)]\n",
    "# order = (p, d, q)\n",
    "\n",
    "# 2. Fit and save your model here:\n",
    "# AR_MODEL = ARIMA(<Process>, order= <order>, enforce_stationarity=False).fit()\n",
    "\n",
    "# 3. Print your model summary here:\n",
    "# print(AR_MODEL.summary())"
   ]
  },
  {
   "source": [
    "\\# 4. Mathematical formulation:\n",
    "\n",
    "$$\\text{AR(2)} := \\quad y_t = 0.3149 y_{t-1} + 0.4699 y_{t-2}$$ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling MA(2)-Process\n",
    "\n",
    "# 1. Define your order here: [order=(p, d, q)]\n",
    "# order = ...\n",
    "\n",
    "# 2. Fit and save your model here:\n",
    "# MA_MODEL = ARIMA(..., ..., enforce_stationarity=False).fit()\n",
    "\n",
    "# 3. Print your model summary here:\n",
    "# print(MA_MODEL.summary())"
   ]
  },
  {
   "source": [
    "\\# 4. Mathematical formulation:\n",
    "\n",
    "$$\\text{MA(2)} := \\quad y_t = $$ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Time Series Analysis (Practice)\n",
    "\n",
    "The goal of this part is to deliver a capable model to predict/forecast the earnings per share of the company *Johnson&Johnson*. The model power will be calculated on an Out-of-Sample file measured by the mean absolute percentage error (MAPE)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task IV:\n",
    "For the first task of the practice part access the data and answer the following question:\n",
    "\n",
    "1. How many rows contains the file?\n",
    "2. What time span is given in the data?\n",
    "3. What frequency has the appearence of the values?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "link_train_set = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/TimeSeries/train_jj.csv'\n",
    "data = pd.read_csv(link_train_set, sep=';')"
   ]
  },
  {
   "source": [
    "## (Task IV) Answer:\n",
    "1. The data contains in sum ... rows.\n",
    "2. The first value is given by the date ... and the last by the date ....\n",
    "3. The frequency is ... ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task V:\n",
    "The next task will refer to the theoretically part of the time series analysis. Visualize the data in its entire length and use the *ACF* and *PACF* to determine the parameter order (*p,d,q*). For your guidance, answer the following questions:\n",
    "\n",
    "1.  Does show the plot of time series any pattern like *trend* or *seasonality*? If yes, what kind of pattern do we have?\n",
    "1.  What do you notice on the *ACF*- and *PACF* -Plots?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the general time series, acf and pacf here:\n"
   ]
  },
  {
   "source": [
    "## (Task V) Answer:\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task VI:\n",
    "One of the most elementary properties is that a time series must be *stationary*. Your next task is to discuss (explain) verbal in your team, why the shown time series is most likely a non-stationary process. In addition to verbal explanation, use an appropriate test-statistic to quantify your assumption.\n",
    "\n",
    "__INFO: Please give the underlying test hypothesis ($H_0 \\quad \\text{vs.} \\quad  H_1$) for your test-statistic__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task VI) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify - stationarity\n"
   ]
  },
  {
   "source": [
    "## Task VII:\n",
    "Refered to your discussion about the stationarity of a time series and your quantifed results, transform the given time series accordingly. Furthermore, repeat the questions from *Task V* on the transformed time series. Any additional information you gained by the transformation process?\n",
    "\n",
    "__INFO: Store the additional transformation in the given dataframe.__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the given time series is a non-stationary process, we need to transform the data. The data is exponentialy increasing over time. So, the first transformation will be the natural logarithm. Furtheremore, the first differention is used to yield a stationary process. Notice, that our d-parameter increases by 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify - stationarity\n"
   ]
  },
  {
   "source": [
    "## (Task VII) Answer:\n",
    "After the transformations the time series is a stationary process and can be used to build up a model. Unfortunately, both autocorrelation plots shows the same characterisic as before and we can not use them to define the parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Model Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task VIII:\n",
    "*\"For the next tasks it is up to you. If you already comfortable with time series analysis jump to the bottom of the notebook and give it a try.\n",
    "Otherwise follow the Tasks in the right order and in right manner :)\"*\n",
    "\n",
    "The following steps must be fullfilled:\n",
    "- Model Selection\n",
    "- In-Sample Evaluation\n",
    "    - MAPE; RMSE; AIC\n",
    "- Out-of-Sample Evaluation\n",
    "- Forecast the next 17 periods (according to the frequency)\n",
    "- Calcualtion of the given Out-of-Sample Test-Set\n",
    "    - MAPE\n",
    "\n",
    "<br>\n",
    "\n",
    "The next goal is to determine the right parameter of the given time series. As we already know, the *ACF* *PACF* does not help very much. But at least, the plots can limit or restrict the range of likely parameter.\n",
    "For the next task refer to the \"stationary\"- *ACF* and *PACF* Plots and choose a rough range of parameter. Calculate then for each combination of parameter the Aika Information Criterion (AIC). Choose your model accordingly and explain your decision.\n",
    "\n",
    "__INFO: Set your range between 0 and 10 for the respective parameter__ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task VIII) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model selection here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best parameter combination based on the Aika Information Criterion (AIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best order of parameter from the your results and fit the model\n"
   ]
  },
  {
   "source": [
    "# Model Evaluation (In-Sample)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task IX:\n",
    "After you have successfully selected your model, it is interesting to know how good your model is. Your next task is to make an in-sample prediction and then determine the following metrics:\n",
    "\n",
    "- MSE \n",
    "- RMSE\n",
    "- MAPE\n",
    "\n",
    "__INFO: You don't need to save the metrics, just use the print()-function__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task IX) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How good is our model? Calculate standard measurements like MSE; RMSE and MAPE\n",
    "# 1. Get the prediction (In-Sample) y_hat\n",
    "\n",
    "\n",
    "# 2. Combine actual values with predictions in a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Print out the Measurements\n"
   ]
  },
  {
   "source": [
    "# Cross Validation (*pseudo* Out-of-Sample)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task X:\n",
    "In the last task, you evaluated your model on the entire time series available. However, how good is your model on unknown data? In the next task, you have to determine your metrics based on different time windows. \n",
    "\n",
    "__INFO: Use the function \"TimeSeriesSplit\" from the Sklearn package__."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task X) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Cross Validation here\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Forecasting "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task XII:\n",
    "To see your *best_model* in action, predict the next 17 periods. Afterwards, save all information in a DataFrame called \"TimeSeriesPredictions\". The DataFrame includes the columns: values(actual values), predictions(In-Sample) and Forecast(Out-of-Sample). Additionaly, visualize your data and the ongoing forecast values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task XII) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast the next 17 periods (OOS)\n",
    "# n_forecast = 17\n",
    "# forecast_series = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesPredictions = data.append(pd.DataFrame(forecast_series))\n",
    "# TimeSeriesPredictions = TimeSeriesPredictions.rename(columns={'predicted_mean': 'Forecast'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize your data\n"
   ]
  },
  {
   "source": [
    "# Data Saving\n",
    "\n",
    "## Info (Google Colab)\n",
    "\n",
    "If you are working in Google Colab, you can save the Results to your Google Drive by running\n",
    "\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "\n",
    "You will be requested to authenticate with your Google Account.\n",
    "\n",
    "The Path to your Google Colab Notebooks Folder will be \"/content/drive/My Drive/Colab Notebooks\".\n",
    "\n",
    "The Commands can then use this Path:\n",
    "\n",
    "```\n",
    "os.makedirs (\"/content/drive/My Drive/Colab Notebooks/Results\", exist_ok=True)\n",
    "df_predictions.to_csv (\"/content/drive/My Drive/Colab Notebooks/Results/TimeSeriesPredictions.csv\", index=False)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task XIII:\n",
    "Save a DataFrame which contains the actual Live Targets as well as the corresponding Live Predictions to a CSV-File.\n",
    "Please write the CSV-File in a way which can be read by a German Microsoft Excel without any necessary Modifications and submit the CSV-File together with your Solution Notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesPredictions.to_csv (os.path.join (DATABRICKS_BASE_DIRECTORY, TEAM_NAME + \"_TimeSeriesPredictions.csv\"), sep=\";\", decimal=\",\", header=True, index=False, encoding=\"utf-8\", float_format=\"%.4f\")\n",
    "# print (\"The Predictions have been successfully saved to a CSV-File, you can download them from:\")\n",
    "# print (DATABRICKS_INSTANCE + \"/files/\" + TEAM_NAME + \"/\" + TEAM_NAME + \"_TimeSeriesPredictions.csv\" + \"?o=\" + DATABRICKS_ORGANISATION)"
   ]
  },
  {
   "source": [
    "## Task XIV: \n",
    "Save your Model to a joblib Pickle Dump File, which can be loaded during Inference. Please submit this File together with your Solution Notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = TEAM_NAME + \"_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_Coding_Challenge_04.joblib\"\n",
    "# dump(best_model, os.path.join(\"/dbfs/FileStore/\" + TEAM_NAME, model_name))\n",
    "# print (\"The fitted Model has been successfully saved, you can download it from:\")\n",
    "# print (DATABRICKS_INSTANCE + \"/files/\" + TEAM_NAME + \"/\" + model_name + \"?o=\" + DATABRICKS_ORGANISATION)"
   ]
  }
 ]
}