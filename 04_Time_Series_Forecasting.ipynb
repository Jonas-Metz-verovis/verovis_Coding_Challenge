{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd09ca9ced1fbb87ca4ccc1db140debb9f236777ff88c014956bbce083d2759c455",
   "display_name": "Python 3.7.10 64-bit ('Tim': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/Jonas-Metz-verovis/verovis_Coding_Challenge/blob/main/04_Time_Series_Forecasting.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introduction - Coding Challenge #4 - Time Series Analysis & Forecasting \n",
    "\n",
    "**Today's coding challenge focuses on time series analysis and time series prediction. In the first part of the challenge, you will demonstrate your theoretical knowledge. In the following section, you have to train your model, evaluate it, and predict the subsequent 17 periods. The data set contains quarterly data of the earnings per share of the company Johnson&Johnson.**\n",
    "\n",
    "**The main goal of the coding challenge is to develop a model that can predict the subsequent 17 earnings per share of Johnson&Johnson.**\n",
    "\n",
    "**The Challenge will be scored based on:**\n",
    "\n",
    "1.  The Prediction Model's Test Mean Absolute Percentage (MAPE) Score\n",
    "1.  The verbal Explanations for specific Processing/Modeling Choices\n",
    "1.  The Readability and Transferability of the submitted Code\n",
    "1.  The Documentation of the submitted Code\n",
    "1.  Optional (not scored): Explanation of the Model's learned Relationships (e.g. through the Feature Importances)\n",
    "\n",
    "General Machine Learning Project Checklist (Focus of this Challenge) by [Aurélien Géron](https://github.com/ageron/handson-ml)\n",
    "\n",
    "1. Frame the Problem and look at the Big Picture\n",
    "1. Get the Data\n",
    "1. Explore the Data to gain Insights\n",
    "1. Prepare the Data to better expose the underlying Data Patterns to the used Machine Learning Algorithms\n",
    "1. Explore many different Models and short-list the best ones\n",
    "1. Fine-tune your Models and combine them into a great Solution\n",
    "1. Present your Solution\n",
    "1. **Launch, monitor, and maintain your Model/Service**\n",
    "\n",
    "**INFO:** Instead of working with [Google Colab](https://colab.research.google.com/), which is recommended because you can get started right away, or [Databricks](https://adb-7072220306909809.9.azuredatabricks.net/?o=7072220306909809), which is recommended if you want to collaborate in real-time, you can also work with your own Development Environment (e.g. [Visual Studio Code](https://code.visualstudio.com/)), by using [Git](https://git-scm.com/) to clone the [verovis Coding Challenge GitHub Repository](https://github.com/Jonas-Metz-verovis/verovis_Coding_Challenge) and collaborate e.g. by using [Microsoft Visual Studio Live Share](https://marketplace.visualstudio.com/items?itemName=MS-vsliveshare.vsliveshare-pack)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Documentation and Support\n",
    "\n",
    "#### The following Resources might be useful to complete this Challenge:\n",
    "\n",
    "1.  [AR-Process (statsmodels)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.ar_model.AutoReg.html#statsmodels.tsa.ar_model.AutoReg)\n",
    "1.  [Forecasting: Principles and Practice (AR-Process)](https://otexts.com/fpp2/AR.html)\n",
    "1.  [Forecasting: Principles and Practice (MA-Process)](https://otexts.com/fpp2/MA.html)\n",
    "1.  [ARIMA-Process (statsmodels)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html#statsmodels.tsa.arima.model.ARIMA)\n",
    "1.  [Forecasting: Principles and Practice (ARIMA-Process)](https://otexts.com/fpp2/arima.html)\n",
    "1.  [Simple Exponential Smoothing (statsmodels)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html#statsmodels.tsa.holtwinters.SimpleExpSmoothing)\n",
    "1.  [Forecasting: Principles and Practice (Simple Exponential Smoothing)](https://otexts.com/fpp2/ses.html)\n",
    "1.  [Holt-Winters (statsmodels)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.Holt.html#statsmodels.tsa.holtwinters.Holt)\n",
    "1.  [Forecasting: Principles and Practice (Holt-Winters)](https://otexts.com/fpp2/holt-winters.html)\n",
    "\n",
    "<hr>\n",
    "\n",
    "1.  [Pandas Documentation](https://pandas.pydata.org/docs/reference/index.html#api)\n",
    "1.  [Numpy Documentation](https://numpy.org/doc/stable/)\n",
    "1.  [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/classes.html)\n",
    "1.  [Seaborn Documentation](https://seaborn.pydata.org/api.html)\n",
    "1.  [SHAP Documentation](https://shap.readthedocs.io/en/latest/api.html)\n",
    "1.  [Pandas Data Wrangling Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "1.  [TowardsDataScience: Data Cleansing](https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d)\n",
    "\n",
    "#### If you don't know how to find a Solution to a given Problem, it often works well if one just \"googles the problem\". Great Sources are:\n",
    "\n",
    "1.  [TowardsDataScience](https://towardsdatascience.com/)\n",
    "1.  [StackOverflow](https://stackoverflow.com/)\n",
    "1.  [Machine Learning Mastery](https://machinelearningmastery.com/start-here/)\n",
    "1.  [Python-Kurs.eu](https://www.python-kurs.eu/python3_kurs.php)\n",
    "1.  [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook)\n",
    "1.  [The Hitchhiker's Guide to Python](https://docs.python-guide.org/)\n",
    "1.  [Overview of Data Science YouTube Channels](https://towardsdatascience.com/top-20-youtube-channels-for-data-science-in-2020-2ef4fb0d3d5)\n",
    "1.  [Introduction to Machine Learning with Python](https://github.com/amueller/introduction_to_ml_with_python) / [Buy the Book](https://www.amazon.de/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)\n",
    "1.  [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf)\n",
    "1.  [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf)\n",
    "1.  [Deep Learning](https://www.deeplearningbook.org/)\n",
    "1.  [Hyndman/Athanasopoulos, Forecasting: Principles and Practice](https://otexts.com/fpp2/)\n",
    "\n",
    "#### This Challenge was created by [Tim Fritzsche](tfritzsche@verovis.com), [Jonas Metz](jmetz@verovis.com) and [Marcel Fynn Froboese](mfroboese@verovis.com), please contact us anytime, if you have any Questions! :-)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Global Flags and Variables\n",
    "Please use the given RANDOM_STATE for all your Models etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Check the Notebook on DataBricks\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# # TODO: Please choose a Team Name!\n",
    "# TEAM_NAME = 'AdminTeam'\n",
    "\n",
    "# DATABRICKS_INSTANCE = \"https://adb-7072220306909809.9.azuredatabricks.net\"\n",
    "# DATABRICKS_ORGANISATION = \"7072220306909809\"\n",
    "# DATABRICKS_BASE_DIRECTORY = os.path.join (\"/dbfs/FileStore\", TEAM_NAME)\n",
    "\n",
    "# MODELS = os.path.join (DATABRICKS_BASE_DIRECTORY, \"Models\")\n",
    "\n",
    "# SAVE_MODEL = True\n",
    "# SAVE_PIPELINE = True"
   ]
  },
  {
   "source": [
    "# Databricks Specifics\n",
    "\n",
    "[Databricks Filestore Documentation](https://docs.databricks.com/data/filestore.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# TODO: Create the config.py file and get sure that it is accessable on DataBricks itself\n",
    "\n",
    "# dbutils.fs.rm (\"/FileStore/\" + TEAM_NAME, recurse=True)\n",
    "# dbutils.fs.mkdirs(\"/FileStore/\" + TEAM_NAME + \"/Models\")\n",
    "# dbutils.fs.ls(\"/FileStore/\" + TEAM_NAME)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Imports\n",
    "\n",
    "### Info (Google Colab)\n",
    "\n",
    "If you are working in Google Colab, you can install necessary (and not already installed) Packages by running e.g.\n",
    "\n",
    "```\n",
    "!pip install shap\n",
    "```\n",
    "\n",
    "### Info (Databricks)\n",
    "\n",
    "If you are working in [Databricks](https://docs.databricks.com/libraries/notebooks-python-libraries.html), you can install necessary (and not already installed) Packages by running e.g. this Command in the first Cell of your Notebook (the Kernel will reset after the Package has been installed):\n",
    "\n",
    "```\n",
    "%pip install shap\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os \n",
    "import config\n",
    "from config import Load_Data\n",
    "from config import Generate_Process\n",
    "from config import plot_process\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# TimeSeries\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.regression.linear_model import yule_walker\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import pacf, acf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 14)\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "source": [
    "# Helper Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ARIMA(endog, ps=range, ds=int, qs=range, return_order=False):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters and corresponding AIC\n",
    "        and the best order direct.\n",
    "        \n",
    "        \n",
    "        endog:      the observed variable\n",
    "        ps:         Order range of parameter p\n",
    "        qs:         Order range of paramter q\n",
    "        ds:         Number of intergrations of the time series\n",
    "\n",
    "    \"\"\"\n",
    "    # Create all possible combination of the given order Parameter\n",
    "    parameters = product(ps, qs)\n",
    "    parameters_list = list(parameters)\n",
    "\n",
    "    # Store all possible combination in order_list\n",
    "    order_list = []\n",
    "    for each in parameters_list:\n",
    "        each = list(each)\n",
    "        each.insert(ds, 1)\n",
    "        each = tuple(each)\n",
    "        order_list.append(each)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # BUG: tqdm maybe don't work on Databricks\n",
    "    for order in tqdm_notebook(order_list):\n",
    "        try: \n",
    "            model = SARIMAX(endog, order=order, simple_differencing=False).fit(disp=False)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Choose AIC as decision metric\n",
    "        aic = model.aic\n",
    "        results.append([order, model.aic])\n",
    "        \n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.columns = ['(p, d, q)', 'AIC']\n",
    "    #Sort in ascending order, lower AIC is better\n",
    "    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    if return_order:\n",
    "        best_order = result_df.iloc[0]['(p, d, q)']\n",
    "        return result_df, best_order\n",
    "\n",
    "    else:\n",
    "        return result_df\n",
    "\n",
    "\n",
    "def model_evaluation(data, best_order=None, ps=range, ds=int, qs=range, n_splits=4):\n",
    "    \"\"\"\n",
    "        This function returns information about key-metrics calcualted on the respective Fold.\n",
    "    \"\"\"\n",
    "    metric_results = {}\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(data['values'])):\n",
    "        # get the train and test data according to the index\n",
    "        train = data['values'][train_index]\n",
    "        test = data['values'][test_index]\n",
    "\n",
    "        # Use the function optimize_ARIMA to define the best order of parameter on the specific train set. \n",
    "        # Then, use the best order (decision based on lowest AIC) to fit a temporarly model and calculate the respective metrics on the respective fold. \n",
    "\n",
    "        print(f\"Start optimizing ARIMA- Parameter on Fold:{fold}\\n\")\n",
    "        result_df = optimize_ARIMA(endog=train, ps=ps, ds=ds, qs=qs, return_order=False)\n",
    "        temp_model = SARIMAX(train, order=best_order, simple_differencing=False)\n",
    "        results = temp_model.fit()\n",
    "\n",
    "        # generate pseudo forecast to compare the actual values with forecast values. The length of the forecast must be the    length of the test! Then calculate the metrics based on the actual values and forecast values in test_set\n",
    "\n",
    "        temp_forecast = results.forecast(len(test))\n",
    "\n",
    "        #For checking if the date index is equal, we join on index and store the dataframes in metric_results\n",
    "        # dfs = pd.DataFrame(temp_forecast).join(pd.DataFrame(test))\n",
    "\n",
    "        metric_results['Fold_'+ str(fold)] = {\n",
    "            'MSE': mean_squared_error(y_true=test.values, y_pred=temp_forecast.values),\n",
    "            'RMSE': np.square(mean_squared_error(y_true=test.values, y_pred=temp_forecast.values)),\n",
    "            'MAE': results.mae,\n",
    "            'MAPE': mean_absolute_percentage_error(y_true=test.values, y_pred=temp_forecast.values),\n",
    "            'SSE': results.sse,\n",
    "            'AIC': results.aic,\n",
    "            'Date_Range_Train': (train.index[0], train.index[-1]),\n",
    "            'Date_Range_Test': (test.index[0], test.index[-1]),\n",
    "            'length_train': int(len(train_index)),\n",
    "            'length_test': int(len(test_index))\n",
    "            }\n",
    "\n",
    "    # Calculate the average over the folds:\n",
    "    df_metric = pd.DataFrame(metric_results)\n",
    "    df_metric['Fold_Mean'] = df_metric.loc[['MSE', 'RMSE', 'MAE', 'MAPE', 'SSE', 'AIC']].mean(axis=1)\n",
    "\n",
    "    return df_metric   \n",
    "\n"
   ]
  },
  {
   "source": [
    "# Data Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_ar_process = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/TimeSeries/Generated_Process/AR_PROCESS.csv'\n",
    "link_ma_process = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/TimeSeries/Generated_Process/MA_PROCESS.csv'\n",
    "link_arma_process = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/TimeSeries/Generated_Process/ARMA_PROCESS.csv'\n",
    "\n",
    "\n",
    "AR_PROCESS = pd.read_csv(link_ar_process, sep=';')\n",
    "MA_PROCESS = pd.read_csv(link_ma_process, sep=';')\n",
    "ARMA_PROCESS = pd.read_csv(link_arma_process, sep=';')"
   ]
  },
  {
   "source": [
    "# Time Series Analysis (Theory)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TASK I: \n",
    "Your first task is to visualize the AR- / MA- and ARMA-Process. Furthermore, try to determine the parameter order for an AR($p$), MA($q$) and an ARMA($p,q$)-Process.\n",
    "To answer this question please use the chunk below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16, 14))\n",
    "\n",
    "# ax1.plot(AR_PROCESS)\n",
    "# plot_acf(AR_PROCESS, ax=ax2, alpha=0.05)\n",
    "# plot_pacf(AR_PROCESS, ax=ax3, alpha=0.05)\n",
    "# plt.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MA_PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARMA_PROCESS"
   ]
  },
  {
   "source": [
    "### (Task I) Answer :\n",
    "To determine the order for an AR($p$)-Process you focus on the *PACF*-Plot. The partial autocorrelation function will give you a guidance which possible lag-Parameter describes the AR($p$)-Process most likely. In this case, the last significant lag-Parameter is $p=2$. So, we can say that it is most likely a AR($p=2$)-Process.\n",
    "\n",
    "<br>\n",
    "\n",
    "To determine the order for an MA($q$)-Process you focus on the *ACF*-Plot. The autocorrelation function will give you a guidance which possible lag-Parameter describes the MA($q$)-Process most likely. In this case, the last significant lag-Parameter is $q=2$. So, we can say that it is most likely a AR($p=2$)-Process.\n",
    "\n",
    "<br>\n",
    "\n",
    "The order for an ARMA($p,q$)-Process is not possible to detect with the *autocorrelation-* or *partial autocorrelation function*. The only information from your functions is that we deal most likely with an process of order $p>0$ and $q>0$. \n",
    "\n",
    "Otherwise, if you can detect the order from the *ACF* or the *PACF* for an ARMA($p,q$)-Process one of the order parmeter must be null since an ARMA-Process is nothing more than composite model. Thus, if you detect the order from the *ACF*, it is an ARMA($p=0, q=k$)-Process where $k$ is the last significant lag. And on the other hand, ARMA($p=k, q=0$) for detecting the parameter over the *PACF*. \n",
    "\n",
    "__In other words:__\n",
    "\n",
    "It is likely to have an ARMA(0,q) or ARIMA(0,d,q) if,\n",
    "-   PACF is exponentially decaying or sinusoidal\n",
    "-   ACF has no significant peak after lag-parameter\n",
    "\n",
    "<br>\n",
    "\n",
    "It is likely to have an ARMA(p,q) or ARIMA(p,d,0) if,\n",
    "-   ACF is exponentially decaying or sinusoidal\n",
    "-   PACF has no significant peak after lag-parameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TASK II:\n",
    "You gained your first information to be able to forecast values of a time series. But not all information lay in the specific visualization. To increase the understanding how an *AR(p)* and a *MA(q)* works define in the following task the mathematical formula with repect to your findings in *Task I*.\n",
    "\n",
    "The formula for an *AR(p)* and *MA(q)* is given by:\n",
    "\n",
    "$$\\text{AR(p)} := \\quad y_t = c + \\sum_{i=1}^{p} \\ \\Phi_i \\ y_{t-i} + \\epsilon_t$$\n",
    "\n",
    "<br>\n",
    "and \n",
    "\n",
    "$$\\text{MA(q)} := \\quad y_t = \\mu + \\epsilon_t + \\Theta_1 \\epsilon_{t-1} + \\Theta_2 \\epsilon_{t-2} + ... + \\Theta_q \\epsilon_{t-q}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task II) Answer:\n",
    "Since we know that we have an AR($p=2$)- and a MA($q=2$)-process, the general mathematical formulation changes for the specific case into:\n",
    "\n",
    "$$\\text{AR(2)} := \\quad y_t = \\Phi_{t-1} y_{t-1} + \\Phi_{t-2} y_{t-2} $$ \n",
    "with $c \\approx 0$.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\text{MA(2)} := \\quad y_t = \\Theta_1 \\epsilon_{t-1} + \\Theta_2 \\epsilon_{t-2}$$ \n",
    "with $\\mu \\approx 0$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task III:\n",
    "As you can see, AR(2)-Process takes the last two time points into account ($y_{t-1}$ and $y_{t-2}$) based on the order we assume. For the MA(2)-Process we take the passed observed error into account ($\\epsilon$). Both Process are depending on $\\Phi$ and $\\Theta$, respectively. The Question is now, how to determine the latter parameter. Obvisouly, this is a simple regression problem. The Task know is to determine both parameter with the help of the *statsmodels* Python package. \n",
    "\n",
    "<br>\n",
    "\n",
    "__Steps:__\n",
    "\n",
    "1.  Define your the *order*-Attribute (Sequence of the prameter is *p,d,q*)\n",
    "1. Fit the model and save it in the variable *AR_MODEL* and *MA_MODEL*, respectively.\n",
    "1. Print out your model summary with the method *<your_fitted_model>.summary()*.\n",
    "1. Use your mathematical formualtion from *Task II* and replace the corresponding parameter. \n",
    "\n",
    "__INFO: For now set your *d*-Parameter to null.__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task III) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling AR(2)-Process\n",
    "\n",
    "# 1. Define your order here: [order=(p, d, q)]\n",
    "# order = (p, d, q)\n",
    "\n",
    "# 2. Fit and save your model here:\n",
    "# AR_MODEL = ARIMA(<Process>, order= <order>, enforce_stationarity=False).fit()\n",
    "\n",
    "# 3. Print your model summary here:\n",
    "# print(AR_MODEL.summary())"
   ]
  },
  {
   "source": [
    "#### # 4. Mathematical formulation:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$$\\text{AR(2)} := \\quad y_t = \\quad ... \\quad y_{t-1} + \\quad...\\quad y_{t-2}$$ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling MA(2)-Process\n",
    "\n",
    "# 1. Define your order here: [order=(p, d, q)]\n",
    "# order = ...\n",
    "\n",
    "# 2. Fit and save your model here:\n",
    "# MA_MODEL = ARIMA(..., ..., enforce_stationarity=False).fit()\n",
    "\n",
    "# 3. Print your model summary here:\n",
    "# print(MA_MODEL.summary())"
   ]
  },
  {
   "source": [
    "#### # 4. Mathematical formulation:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\\# 4. Mathematical formulation:\n",
    "\n",
    "$$\\text{MA(2)} := \\quad y_t = \\quad ... \\quad \\epsilon_{t-1} + \\quad ... \\quad  \\epsilon_{t-2}$$ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Time Series Analysis (Practice)\n",
    "\n",
    "The goal of this part is to deliver a capable model to predict/forecast the earnings per share of the company *Johnson&Johnson*. The model power will be calculated on an Out-of-Sample file measured by the mean absolute percentage error (MAPE)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task IV:\n",
    "For the first task of the practice part access the data and answer the following question:\n",
    "\n",
    "1. How many rows contains the file?\n",
    "2. What time span is given in the data?\n",
    "3. What frequency has the appearence of the values?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "link_train_set = 'https://raw.githubusercontent.com/Jonas-Metz-verovis/verovis_Coding_Challenge/main/Data/TimeSeries/train_jj.csv'\n",
    "data = pd.read_csv(link_train_set, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here!\n"
   ]
  },
  {
   "source": [
    "## (Task IV) Answer:\n",
    "1. The data contains in sum ... rows.\n",
    "2. The first value is given by the date ... and the last by the date ....\n",
    "3. The frequency is ... ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task V:\n",
    "The next task will refer to the theoretically part of the time series analysis. Visualize the data in its entire length and use the *ACF* and *PACF* to determine the parameter order (*p,d,q*). For your guidance, answer the following questions:\n",
    "\n",
    "1.  Does show the plot of time series any pattern like *trend* or *seasonality*? If yes, what kind of pattern do we have?\n",
    "1.  What do you notice on the *ACF*- and *PACF* -Plots?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the general time series, acf and pacf here:\n",
    "# # Plot the general time series here:\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16, 14))\n",
    "\n",
    "# ax1.plot(data['values'])\n",
    "# plt.title('Quarterly EPS for Johnson & Johnson')\n",
    "# plt.ylabel('EPS per share ($)')\n",
    "# plt.xlabel('Date')\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# # Use of acf and pacf\n",
    "# plot_acf(data['values'], ax=ax2)\n",
    "# plot_pacf(data['values'], ax=ax3)\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "source": [
    "## (Task V) Answer:\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task VI:\n",
    "One of the most elementary properties is that a time series must be *stationary*. Your next task is to discuss (explain) verbal in your team, why the shown time series is most likely a non-stationary process. In addition to verbal explanation, use an appropriate test-statistic to quantify your assumption."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task VI) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quantify - stationarity\n",
    "# # Quantify - stationarity\n",
    "# adf_result = adfuller(data['values'])\n",
    "# print('The used test statistic is called the ADF-Test (Augemented Dickey Fuller). H_0: non-stationary process vs H_1: stationary process')\n",
    "# print('\\n-------------------------\\n')\n",
    "# print('Test-Statistics of the ADF-Test\\n')\n",
    "# print(f'ADF-Statistic: {adf_result[0]}')\n",
    "# print(f'p-value: {adf_result[1]}')"
   ]
  },
  {
   "source": [
    "## Task VII:\n",
    "Refered to your discussion about the stationarity of a time series and your quantifed results, transform the given time series accordingly. Furthermore, repeat the questions from *Task V* on the transformed time series. Any additional information you gained by the transformation process?\n",
    "\n",
    "__INFO: Store the additional transformation in the given dataframe.__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the given time series is a non-stationary process, we need to transform the data. The data is exponentialy increasing over time. So, the first transformation will be the natural logarithm. Furtheremore, the first differention is used to yield a stationary process. Notice, that our d-parameter increases by 1.\n",
    "\n",
    "############# Your Code #############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############# End of your Code #######\n",
    "\n",
    "############ Repeat steps from Task VI ################\n",
    "\n",
    "# # Plot your transformed Data\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16, 14))\n",
    "\n",
    "# ax1.plot(<your new transformation>)\n",
    "# plt.title('Quarterly EPS for Johnson & Johnson')\n",
    "# plt.ylabel(<y label>)\n",
    "# plt.xlabel(< x label)\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# # Use of acf and pacf\n",
    "# plot_acf(<your new transformation>, ax=ax2)\n",
    "# plot_pacf(<your new transformation>, ax=ax3)\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quantify - stationarity\n",
    "\n",
    "# adf_result = adfuller(<your new transformation>)\n",
    "\n",
    "# print('The used test statistic is called the ADF-Test (Augemented Dickey Fuller). H_0: non-stationar process vs H_1: stationar process')\n",
    "# print('\\n-------------------------\\n')\n",
    "# print('Test-Statistics of the ADF-Test\\n')\n",
    "# print(f'ADF-Statistic: {adf_result[0]}')\n",
    "# print(f'p-value: {adf_result[1]}')"
   ]
  },
  {
   "source": [
    "## (Task VII) Answer:\n",
    "After the transformations the time series is a stationary process and can be used to build up a model. Unfortunately, both autocorrelation plots shows the same characterisic as before and we can not use them to define the parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Model Selection (Parameter optimization)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task VIII:\n",
    "*\"For the next tasks it is up to you. If you already comfortable with time series analysis jump to the bottom of the notebook and give it a try.\n",
    "Otherwise follow the Tasks in the right order and in right manner :)\"*\n",
    "\n",
    "The following steps must be fullfilled:\n",
    "- Model Selection\n",
    "- Model evaluation on the available time series\n",
    "    - MAPE\n",
    "    - RMSE\n",
    "    - AIC\n",
    "    - and other metrics if you like :)\n",
    "\n",
    "- Evaluation of model performance from a retrospective perspective. Means, develop critical metrics on time fragments of your time series\n",
    "    - MAPE\n",
    "    - RMSE\n",
    "    - AIC\n",
    "    - and other metrics if you like :)\n",
    "\n",
    "- Forecast the next 17 periods (according to the frequency), visualize the forecast and discuss what your forecast means in the business context.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The next goal is to determine the right parameter of the given time series. As we already know, the *ACF* *PACF* does not help very much. But at least, the plots can limit or restrict the range of likely parameter.\n",
    "For the next task refer to the \"stationary\"- *ACF* and *PACF* Plots and choose a rough range of parameter. Calculate then for each combination of parameter the Aika Information Criterion (AIC). Choose your model accordingly and explain your decision.\n",
    "\n",
    "__INFO: If you don't know how to handle this task have a look at the *Helper-Function* at the beginning of this Notebook.__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task VIII) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best parameter combination based on the Aika Information Criterion (AIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best order of parameter from the your results and fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Model evaluation on the available time series"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task IX:\n",
    "After you have successfully selected your model, it is interesting to know how good your model is. Your next task is to make an in-sample prediction and then determine the following metrics:\n",
    "\n",
    "- MSE \n",
    "- RMSE\n",
    "- MAPE\n",
    "\n",
    "__INFO: You don't need to save the metrics, just use the print()-function__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task IX) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How good is our model? Calculate standard measurements like MSE; RMSE and MAPE\n",
    "# 1. Get the prediction. Store the reusults in a pandas Series called \"prediction_series\"\n",
    "\n",
    "# prediction_series = \n",
    "\n",
    "# 2. Combine actual values with predictions in a DataFrame\n",
    "\n",
    "# data['Predictions'] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Print out the Measurements\n",
    "# print(f'Mean Squared Error (MSE): {round(mean_squared_error(data[\"values\"], data[\"Predictions\"]),4)}')\n",
    "# print(f'Root Mean Squared Error (RMSE): {round(np.square(mean_squared_error(data[\"values\"], data[\"Predictions\"])),4)}')\n",
    "# print(f'Mean Absolute Percentage Error (MAPE): {round(mean_absolute_percentage_error(data[\"values\"], data[\"Predictions\"]) *100, 4)}%')"
   ]
  },
  {
   "source": [
    "# Cross Validation \n",
    "## Evaluation of the model performance from a retrospective perspective. Means, develop critical metrics on time fragments of your time series to determine how good your model really is."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task X:\n",
    "In the last task, you evaluated your model on the entire time series available. However, how good is your model on *unknown data*? In the next task, you have to determine your metrics based on different time windows. \n",
    "\n",
    "Store the calculated metrics on each fold and deliver a DataFrame to see how your model would perform on the respective time window (*How many folds you choose is up to you*).\n",
    "\n",
    "__INFO: Use the function \"TimeSeriesSplit\" from the Sklearn package. If you don't know how to handle this task have a look at the *Helper-Function model_evaluation*.__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task X) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Cross Validation here\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Forecasting "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task XI:\n",
    "To see your *best_model* in action, predict the next 17 periods. Afterwards, save all information in a DataFrame called \"TimeSeriesPredictions\". The DataFrame includes the columns: values(actual values), predictions(In-Sample) and Forecast(Out-of-Sample). Additionaly, visualize your data the in-sample prediction and the ongoing forecast values.\n",
    "\n",
    "What are your predictions, and what conclusions do you draw from them? Discuss your prognosis in the context of the business. What recommended action can be derived from the model, and is the recommended action valid?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Task XI) Answer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast the next 17 periods (OOS)\n",
    "\n",
    "# n_forecast = ...\n",
    "\n",
    "# forecast_series = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesPredictions = data.append(pd.DataFrame(forecast_series))\n",
    "# TimeSeriesPredictions = TimeSeriesPredictions.rename(columns={'predicted_mean': 'Forecast'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesPredictions.head()\n",
    "# TimeSeriesPredictions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize your data\n"
   ]
  },
  {
   "source": [
    "#### Discussion\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Saving\n",
    "\n",
    "## Info (Google Colab)\n",
    "\n",
    "If you are working in Google Colab, you can save the Results to your Google Drive by running\n",
    "\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "\n",
    "You will be requested to authenticate with your Google Account.\n",
    "\n",
    "The Path to your Google Colab Notebooks Folder will be \"/content/drive/My Drive/Colab Notebooks\".\n",
    "\n",
    "The Commands can then use this Path:\n",
    "\n",
    "```\n",
    "os.makedirs (\"/content/drive/My Drive/Colab Notebooks/Results\", exist_ok=True)\n",
    "df_predictions.to_csv (\"/content/drive/My Drive/Colab Notebooks/Results/TimeSeriesPredictions.csv\", index=False)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task XII:\n",
    "Save a DataFrame which contains the actual Live Targets as well as the corresponding Live Predictions to a CSV-File.\n",
    "Please write the CSV-File in a way which can be read by a German Microsoft Excel without any necessary Modifications and submit the CSV-File together with your Solution Notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesPredictions.to_csv (os.path.join (DATABRICKS_BASE_DIRECTORY, TEAM_NAME + \"_TimeSeriesPredictions.csv\"), sep=\";\", decimal=\",\", header=True, index=False, encoding=\"utf-8\", float_format=\"%.4f\")\n",
    "# print (\"The Predictions have been successfully saved to a CSV-File, you can download them from:\")\n",
    "# print (DATABRICKS_INSTANCE + \"/files/\" + TEAM_NAME + \"/\" + TEAM_NAME + \"_TimeSeriesPredictions.csv\" + \"?o=\" + DATABRICKS_ORGANISATION)"
   ]
  },
  {
   "source": [
    "## Task XIII: \n",
    "Save your Model to a joblib Pickle Dump File, which can be loaded during Inference. Please submit this File together with your Solution Notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = TEAM_NAME + \"_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_Coding_Challenge_04.joblib\"\n",
    "# dump(best_model, os.path.join(\"/dbfs/FileStore/\" + TEAM_NAME, model_name))\n",
    "# print (\"The fitted Model has been successfully saved, you can download it from:\")\n",
    "# print (DATABRICKS_INSTANCE + \"/files/\" + TEAM_NAME + \"/\" + model_name + \"?o=\" + DATABRICKS_ORGANISATION)"
   ]
  }
 ]
}